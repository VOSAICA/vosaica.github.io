{"pages":[],"posts":[{"title":"Shadertoy学习 vol3. 抗锯齿，SRGB，镜面反射","text":"书接上文，这次我们来实现更多的渲染功能。本文完整代码 抗锯齿这个内容比较简单，因为我们的渲染器是多帧合成的，所以在每一帧都对像素添加随机的位移就可以了。我们这里随机的位移范围是-0.5到+0.5 找到： 1vec3 rayTarget =vec3((gl_FragCoord.xy/iResolution.xy) * 2.0f - 1.0f, cameraDistance); 改为： 123vec2 jitter = vec2(RandomFloat01(rngState), RandomFloat01(rngState)) - 0.5f;vec3 rayTarget =vec3(((gl_FragCoord.xy + jitter) / iResolution.xy) * 2.0f - 1.0f, cameraDistance); SRGB由于人眼对颜色的感觉不是线性的，所以我们要引入SRGB颜色空间。SRGB里的颜色并不是线性间隔的，深色的值比较多，而浅色的较少。 我们的颜色通道计算不能采用非线性值，所以我们会在最后输出像素颜色时再把颜色转换为SRGB。 这里我们可以在Shadertoy中添加一个commom选项卡，又或者在Shader Toy中新建一个frag文件，他们的作用跟头文件一样。 这里我新建了一个Test4Common.frag，并在其他两个文件的开头添加了如下代码: 1#include \"Test4Common.frag\" 这样，我们的shader便可以调用别的文件里定义的函数了。我们也可以把之前的常量全部移动到这个头文件里。 以下是将线性颜色转换为SRBG颜色的代码: 12345678910111213141516171819202122232425262728293031323334353637vec3 LessThan(vec3 f, float value){ return vec3 ( (f.x &lt; value) ? 1.0f : 0.0f, (f.y &lt; value) ? 1.0f : 0.0f, (f.z &lt; value) ? 1.0f : 0.0f );}vec3 Linear2SRGB(vec3 rgb){ rgb = clamp(rgb, 0.0f, 1.0f); return mix ( pow(rgb, vec3(1.0f / 2.4f)) * 1.055f - 0.055f, rgb * 12.92f, LessThan(rgb, 0.0031308f) );}vec3 SRGB2Linear(vec3 rgb){ return rgb; rgb = clamp(rgb, 0.0f, 1.0f); return mix ( pow(((rgb + 0.055f) / 1.055f), vec3(2.4f)), rgb / 12.92f, LessThan(rgb, 0.04045f) );} 这部分的原理不必过分深究，因为它更多是人为制定的规则，这是微软提供的对颜色空间转换的解释:要记住的是最好不要在渲染时引入SRGB色彩空间，因为有可能会导致渲染出现问题。 曝光和色调映射当我们计算光照的时候，光线的强度可以从0一直到无限，可像素的值却只能停在0到1之间。 为此，我们需要把值重新映射到0到1的区间，使暗处可以保留细节，同时也能看清楚亮处。 我们在头文件当中添加以下代码： 123456789vec3 ACESFilm(vec3 x){ float a = 2.51f; float b = 0.03f; float c = 2.43f; float d = 0.59f; float e = 0.14f; return clamp((x * (a * x + b)) / (x * (c * x + d) + e), 0.0f, 1.0f);} 这个函数其实很好理解，就跟线性插值差不多。 之后再修改主函数: 12color *= c_exposure;color = ACESFilm(color); 注意我们在这里额外添加了一个c_exposure常量，用于控制曝光，我在这里设置的值为0.5. 你也可以手动修改来得到一个满意的亮度。 测试一下，看看效果! 镜面反射现在到了大头了啊，我们要给渲染器加入镜面反射了。 GLSL是有一个reflect()函数已经被定义好的了，你只需要给出入射的光线和法线，就能返回出射光线了。 但在那之前，我们需要对SRayHitInfo的这个结构体做出一些修改。 我们先定义一个SMaterialInfo结构体，把albedo, emissive都移过来，再添加三个新的变量：specularColor，percentSpecular和roughness 再把SMaterialInfo添加到SRayHitInfo里: 123456789101112131415struct SMaterialInfo{ vec3 albedo; vec3 emissive; vec3 specularColor; float percentSpecular; float roughness;};struct SRayHitInfo{ float dist; vec3 normal; SMaterialInfo material;}; specularColor描述了镜面反射后光线的颜色，它和albedo类似，不过一个物体的漫反射颜色和镜面反射颜色不一定相同。 percentageSpecular是一个float，取值范围在0到1之间，描述了镜面反射光线所占全部反射光线的比值，值越低，漫反射所占比重越高，反之亦然 roughness也是取值范围在0到1之间的float，绝对光滑物体的roughness为0。 计算时的原理如下: 我们会随机生成一个0到1之间的小数，如果它小于percentageSpecular,则让光线进行镜面反射，否则进行漫反射。 roughness为1的进行漫反射。 roughness为0的用reflect()计算出射方向。 roughness在0到1之间的就先平方roughness，再线性插值漫反射和镜面反射的方向来算出反射角度，最后再归一化。 对于镜面反射，我们不再乘上albedo，而是specularColor。 其实这个逻辑还是比较好理解的, 然后我们就可以开始写了，找到代码: 1234567rayPos = (rayPos + rayDir * hitInfo.dist) + hitInfo.normal * c_rayPosNormalNudge;rayDir = normalize(hitInfo.normal + RandomUnitVector(rngState));ret += hitInfo.emissive * throughput;throughput *= hitInfo.albedo; 更改为: 1234567891011121314rayPos = (rayPos + rayDir * hitInfo.dist) + hitInfo.normal * c_rayPosNormalNudge; //Wether or not to do the specular reflection ray float doSpecular = (RandomFloat01(rngState) &lt; hitInfo.material.percentSpecular) ? 1.0f : 0.0f; vec3 diffuseRayDir = normalize(hitInfo.normal + RandomUnitVector(rngState)); vec3 specularRayDir = reflect(rayDir, hitInfo.normal); specularRayDir = normalize(mix(specularRayDir, diffuseRayDir, hitInfo.material.roughness * hitInfo.material.roughness)); rayDir = mix(diffuseRayDir, specularRayDir, doSpecular); ret += hitInfo.material.emissive * throughput; throughput *= mix(hitInfo.material.albedo, hitInfo.material.specularColor, doSpecular); 最后更改一下场景，要记得添加上新的Material属性： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179void TestSceneTrace(in vec3 rayPos, in vec3 rayDir, inout SRayHitInfo hitInfo){ // to move the scene around, since we can't move the camera yet vec3 sceneTranslation = vec3(0.0f, 0.0f, 10.0f); vec4 sceneTranslation4 = vec4(sceneTranslation, 0.0f); // back wall { vec3 A = vec3(-12.6f, -12.6f, 25.0f) + sceneTranslation; vec3 B = vec3( 12.6f, -12.6f, 25.0f) + sceneTranslation; vec3 C = vec3( 12.6f, 12.6f, 25.0f) + sceneTranslation; vec3 D = vec3(-12.6f, 12.6f, 25.0f) + sceneTranslation; if (TestQuadTrace(rayPos, rayDir, hitInfo, A, B, C, D)) { hitInfo.material.albedo = vec3(0.7f, 0.7f, 0.7f); hitInfo.material.emissive = vec3(0.0f, 0.0f, 0.0f); hitInfo.material.percentSpecular = 0.0f; hitInfo.material.roughness = 0.0f; hitInfo.material.specularColor = vec3(0.0f, 0.0f, 0.0f); } } // floor { vec3 A = vec3(-12.6f, -12.45f, 25.0f) + sceneTranslation; vec3 B = vec3( 12.6f, -12.45f, 25.0f) + sceneTranslation; vec3 C = vec3( 12.6f, -12.45f, 15.0f) + sceneTranslation; vec3 D = vec3(-12.6f, -12.45f, 15.0f) + sceneTranslation; if (TestQuadTrace(rayPos, rayDir, hitInfo, A, B, C, D)) { hitInfo.material.albedo = vec3(0.7f, 0.7f, 0.7f); hitInfo.material.emissive = vec3(0.0f, 0.0f, 0.0f); hitInfo.material.percentSpecular = 0.0f; hitInfo.material.roughness = 0.0f; hitInfo.material.specularColor = vec3(0.0f, 0.0f, 0.0f); } } // cieling { vec3 A = vec3(-12.6f, 12.5f, 25.0f) + sceneTranslation; vec3 B = vec3( 12.6f, 12.5f, 25.0f) + sceneTranslation; vec3 C = vec3( 12.6f, 12.5f, 15.0f) + sceneTranslation; vec3 D = vec3(-12.6f, 12.5f, 15.0f) + sceneTranslation; if (TestQuadTrace(rayPos, rayDir, hitInfo, A, B, C, D)) { hitInfo.material.albedo = vec3(0.7f, 0.7f, 0.7f); hitInfo.material.emissive = vec3(0.0f, 0.0f, 0.0f); hitInfo.material.percentSpecular = 0.0f; hitInfo.material.roughness = 0.0f; hitInfo.material.specularColor = vec3(0.0f, 0.0f, 0.0f); } } // left wall { vec3 A = vec3(-12.5f, -12.6f, 25.0f) + sceneTranslation; vec3 B = vec3(-12.5f, -12.6f, 15.0f) + sceneTranslation; vec3 C = vec3(-12.5f, 12.6f, 15.0f) + sceneTranslation; vec3 D = vec3(-12.5f, 12.6f, 25.0f) + sceneTranslation; if (TestQuadTrace(rayPos, rayDir, hitInfo, A, B, C, D)) { hitInfo.material.albedo = vec3(0.7f, 0.1f, 0.1f); hitInfo.material.emissive = vec3(0.0f, 0.0f, 0.0f); hitInfo.material.percentSpecular = 0.0f; hitInfo.material.roughness = 0.0f; hitInfo.material.specularColor = vec3(0.0f, 0.0f, 0.0f); } } // right wall { vec3 A = vec3( 12.5f, -12.6f, 25.0f) + sceneTranslation; vec3 B = vec3( 12.5f, -12.6f, 15.0f) + sceneTranslation; vec3 C = vec3( 12.5f, 12.6f, 15.0f) + sceneTranslation; vec3 D = vec3( 12.5f, 12.6f, 25.0f) + sceneTranslation; if (TestQuadTrace(rayPos, rayDir, hitInfo, A, B, C, D)) { hitInfo.material.albedo = vec3(0.1f, 0.7f, 0.1f); hitInfo.material.emissive = vec3(0.0f, 0.0f, 0.0f); hitInfo.material.percentSpecular = 0.0f; hitInfo.material.roughness = 0.0f; hitInfo.material.specularColor = vec3(0.0f, 0.0f, 0.0f); } } // light { vec3 A = vec3(-5.0f, 12.4f, 22.5f) + sceneTranslation; vec3 B = vec3( 5.0f, 12.4f, 22.5f) + sceneTranslation; vec3 C = vec3( 5.0f, 12.4f, 17.5f) + sceneTranslation; vec3 D = vec3(-5.0f, 12.4f, 17.5f) + sceneTranslation; if (TestQuadTrace(rayPos, rayDir, hitInfo, A, B, C, D)) { hitInfo.material.albedo = vec3(0.0f, 0.0f, 0.0f); hitInfo.material.emissive = vec3(1.0f, 0.9f, 0.7f) * 20.0f; hitInfo.material.percentSpecular = 0.0f; hitInfo.material.roughness = 0.0f; hitInfo.material.specularColor = vec3(0.0f, 0.0f, 0.0f); } } if (TestSphereTrace(rayPos, rayDir, hitInfo, vec4(-9.0f, -9.3f, 20.0f, 3.0f)+sceneTranslation4)) { hitInfo.material.albedo = vec3(0.9f, 0.9f, 0.5f); hitInfo.material.emissive = vec3(0.0f, 0.0f, 0.0f); hitInfo.material.percentSpecular = 0.1f; hitInfo.material.roughness = 0.2f; hitInfo.material.specularColor = vec3(0.9f, 0.9f, 0.9f); } if (TestSphereTrace(rayPos, rayDir, hitInfo, vec4(0.0f, -9.3f, 20.0f, 3.0f)+sceneTranslation4)) { hitInfo.material.albedo = vec3(0.9f, 0.5f, 0.9f); hitInfo.material.emissive = vec3(0.0f, 0.0f, 0.0f); hitInfo.material.percentSpecular = 0.3f; hitInfo.material.roughness = 0.2; hitInfo.material.specularColor = vec3(0.9f, 0.9f, 0.9f); } // a ball which has blue diffuse but red specular. an example of a \"bad material\". // a better lighting model wouldn't let you do this sort of thing if (TestSphereTrace(rayPos, rayDir, hitInfo, vec4(9.0f, -9.3f, 20.0f, 3.0f)+sceneTranslation4)) { hitInfo.material.albedo = vec3(0.0f, 0.0f, 1.0f); hitInfo.material.emissive = vec3(0.0f, 0.0f, 0.0f); hitInfo.material.percentSpecular = 0.5f; hitInfo.material.roughness = 0.4f; hitInfo.material.specularColor = vec3(1.0f, 0.0f, 0.0f); } // shiny green balls of varying roughnesses { if (TestSphereTrace(rayPos, rayDir, hitInfo, vec4(-10.0f, 0.0f, 23.0f, 1.75f)+sceneTranslation4)) { hitInfo.material.albedo = vec3(1.0f, 1.0f, 1.0f); hitInfo.material.emissive = vec3(0.0f, 0.0f, 0.0f); hitInfo.material.percentSpecular = 1.0f; hitInfo.material.roughness = 0.0f; hitInfo.material.specularColor = vec3(0.3f, 1.0f, 0.3f); } if (TestSphereTrace(rayPos, rayDir, hitInfo, vec4(-5.0f, 0.0f, 23.0f, 1.75f)+sceneTranslation4)) { hitInfo.material.albedo = vec3(1.0f, 1.0f, 1.0f); hitInfo.material.emissive = vec3(0.0f, 0.0f, 0.0f); hitInfo.material.percentSpecular = 1.0f; hitInfo.material.roughness = 0.25f; hitInfo.material.specularColor = vec3(0.3f, 1.0f, 0.3f); } if (TestSphereTrace(rayPos, rayDir, hitInfo, vec4(0.0f, 0.0f, 23.0f, 1.75f)+sceneTranslation4)) { hitInfo.material.albedo = vec3(1.0f, 1.0f, 1.0f); hitInfo.material.emissive = vec3(0.0f, 0.0f, 0.0f); hitInfo.material.percentSpecular = 1.0f; hitInfo.material.roughness = 0.5f; hitInfo.material.specularColor = vec3(0.3f, 1.0f, 0.3f); } if (TestSphereTrace(rayPos, rayDir, hitInfo, vec4(5.0f, 0.0f, 23.0f, 1.75f)+sceneTranslation4)) { hitInfo.material.albedo = vec3(1.0f, 1.0f, 1.0f); hitInfo.material.emissive = vec3(0.0f, 0.0f, 0.0f); hitInfo.material.percentSpecular = 1.0f; hitInfo.material.roughness = 0.75f; hitInfo.material.specularColor = vec3(0.3f, 1.0f, 0.3f); } if (TestSphereTrace(rayPos, rayDir, hitInfo, vec4(10.0f, 0.0f, 23.0f, 1.75f)+sceneTranslation4)) { hitInfo.material.albedo = vec3(1.0f, 1.0f, 1.0f); hitInfo.material.emissive = vec3(0.0f, 0.0f, 0.0f); hitInfo.material.percentSpecular = 1.0f; hitInfo.material.roughness = 1.0f; hitInfo.material.specularColor = vec3(0.3f, 1.0f, 0.3f); } }} 看一下效果： 真不错 写累了，来首歌放松一下: siqlo · Dual Personality","link":"/2020/08/11/Shadertoy_vol3/"},{"title":"Shadertoy学习 vol1. 基本介绍与环境搭建","text":"Link for the Seascape在知乎上看到这个海洋的着色器时，我被震撼到了。立马去找了出处，然后就发现了Shadertoy这个网站。虽然之前在学习Unity的Shader Graph的时候也听说过，不过没想到有人在上面实现了这么好的效果。所以呢，自己便开始琢磨琢磨，看看能不能学习一下。 Shadertoy的基本原理在网站里随便点开一个Shader，就能在右边看到它的代码。这些代码是用openGL的着色器语言—GLSL写的，基本的语法跟C++差不多，不过在一些地方会有出入，比如增加了一些向量和数学的方法，还有一些变量修饰符之类的。 Shadertoy并不是一个完整的着色器，它只是一个片元着色器，不涉及顶点的着色和变换，所以在这里你看不见他们导入的模型和Mesh。这里不少3D物体都是靠Ray Marching和SDF函数实现的，所以会和大家导入一个3D模型再编写Material和Shader的流程有些不同。 当然在实际应用时，在片元着色器里实现3D效果的方法基本用不到，如果想要为了掌握3D游戏里的Shader编写，大家还是去UE或者Unity里玩吧，干嘛费这个劲。 在知道了以上这些基本知识后，如果你仍对Shadertoy抱有兴趣的话，就可以接着往后看了 环境搭建Shadertoy依赖的是openGL的图形api，在浏览器中则多半是webGL，在网站中便可以直接使用。点击右上角的新建，就可以制作你自己的Shader了。 那对于一些不想在网页端实验的同学呢，也有很方便的解决方案，就是去vscode里下一个叫做Shader Toy的插件，再下一个对GLSL语言的支持。 不过要注意的是vscode里的Shader Toy和Shadertoy网站上一部分的变量命名会略有不同，大家可以在文档里看一下。 在之后的教程中，我的代码会以vscode当中的Shader Toy作为标准，所以在Shadertoy上是不能直接运行的，要做出一些修改。 在安装好这两个插件后新建一个文件，命名为test1.frag粘贴以下代码： 12345void main(){ vec2 st = gl_FragCoord.xy/iResolution.xy; gl_FragColor = vec4(st.x,st.y,cos(iTime),1.0);} 保存后右键，选择Shader Toy: Show GLSL Preview 然后你应该就能看到一张会变换颜色的图片了。","link":"/2020/08/07/Shadertoy_vol1/"},{"title":"Shadertoy学习 vol4. 菲涅尔, 折射与吸收, 环绕相机","text":"前排提醒，这次的内容会比较多，同时会对之前的代码做出大量修改。另外，这是光追渲染器的最后一篇了，下次估计开坑ray marching。 菲涅尔 简单来说，菲涅尔效应是物体在不同观察角度下，表面反射比率不同的现象。具体的效果取决于物体本身的物理特性。模拟菲涅尔可以增强物体材质的真实感。 下面这个函数实现了菲涅尔效果： 123456789101112131415161718192021float FresnelReflectAmount(float n1, float n2, vec3 normal, vec3 incident, float f0, float f90){ // Schlick aproximation float r0 = (n1-n2) / (n1+n2); r0 *= r0; float cosX = -dot(normal, incident); if (n1 &gt; n2) { float n = n1/n2; float sinT2 = n*n*(1.0-cosX*cosX); // Total internal reflection if (sinT2 &gt; 1.0) return f90; cosX = sqrt(1.0-sinT2); } float x = 1.0-cosX; float ret = r0+(1.0-r0)*x*x*x*x*x; // adjust reflect multiplier for object reflectivity return mix(f0, f90, ret);} 这里n1是入射光线材质的折射率(IOR), n2是被击中的对象材质的折射率,normal是光线碰撞处的表面法线, incident是光线集中对象时的方向, f0是对象的最小反射率(当光线与法线呈0°时), f90是对象的最大反射率(当光线与法线呈90°时). 当应用到我们的渲染器中时，找到这段代码： 12//Wether or not to do the specular reflection ray float doSpecular = (RandomFloat01(rngState) &lt; hitInfo.material.percentSpecular) ? 1.0f : 0.0f; 改为: 123456789101112// apply fresnelfloat specularChance = hitInfo.material.percentSpecular;if (specularChance &gt; 0.0f){ specularChance = FresnelReflectAmount( 1.0, hitInfo.material.IOR, rayDir, hitInfo.normal, hitInfo.material.percentSpecular, 1.0f); } // calculate whether we are going to do a diffuse or specular reflection ray float doSpecular = (RandomFloat01(rngState) &lt; specularChance) ? 1.0f : 0.0f; 注意还要向SMaterialInfo添加IOR 大家可以根据在网上找到的物体IOR来为你的渲染添加更真实的材质: 这是菲涅尔效果的演示，从左到右IOR从1增加到2: 折射和吸收让我们先向SMaterialInfo添加更多的属性，找到结构体，改为： 123456789101112struct SMaterialInfo{ vec3 albedo; vec3 emissive; vec3 specularColor; float specularChance; float specularRoughness; float IOR; float refractionChance; float refractionRoughness; vec3 refractionColor;}; 现在我们有了镜面反射概率，折射概率，以及一个漫反射概率。漫反射概率为1.0 - specularChance - refractionChance, 并没有出现在结构体中，因为我们默认的光线反射方式就是漫反射。 因为我们的SmaterialInfo有了很多属性，很有可能忘记初始化其中一个造成问题，所以要来写一个初始化函数： 1234567891011121314SMaterialInfo GetZeroedMaterial(){ SMaterialInfo ret; ret.albedo = vec3(0.0f, 0.0f, 0.0f); ret.emissive = vec3(0.0f, 0.0f, 0.0f); ret.specularChance = 0.0f; ret.specularRoughness = 0.0f; ret.specularColor = vec3(0.0f, 0.0f, 0.0f); ret.IOR = 1.0f; ret.refractionChance = 0.0f; ret.refractionRoughness = 0.0f; ret.refractionColor = vec3(0.0f, 0.0f, 0.0f); return ret;} 之后每次新建了一个Material都可以调用这个函数来初始化. 接着向SRayHitInfo添加一个新的属性,叫fromInside 1234567struct SRayHitInfo{ float dist; vec3 normal; bool fromInside; SMaterialInfo material;}; 因为我们现在有透明物体, 所以光线会从物体内部碰到表面. 我们需要知道碰撞究竟是在内部发生的还是在外部发生的. 当然, 我们的长方形是没有内部的,所以在TestQuadTrace里把fromInside直接设成false就可以. 不过在TestSphereTrace里还是要修改这个fromInside的值的, 之前代码里已经有判断的逻辑了, 只要再给它赋个值就可以了. 这里用到比尔-朗伯定律来实现的光线衰减, 给光线乘上系数:\\(Multiplier=e^{\\left( -absorb \\cdot distance \\right) }\\) 之后对于逻辑的修改内容比较多, 大家可以直接参考代码来理解, 如有不理解的地方欢迎在评论区留言 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788899091929394959697vec3 GetColorForRay(in vec3 startRayPos, in vec3 startRayDir, inout uint rngState){ vec3 ret = vec3(0.0f, 0.0f, 0.0f); vec3 throughput = vec3(1.0f, 1.0f, 1.0f); vec3 rayPos = startRayPos; vec3 rayDir = startRayDir; for (int bounceIndex = 0; bounceIndex &lt;= c_numBounces; ++bounceIndex) { SRayHitInfo hitInfo; hitInfo.material = GetZeroedMaterial(); hitInfo.dist = c_superFar; hitInfo.fromInside = false; TestSceneTrace(rayPos, rayDir, hitInfo); if (hitInfo.dist == c_superFar) { ret = vec3(0.01f, 0.01f, 0.01f); break; } if (hitInfo.fromInside) throughput *= exp(-hitInfo.material.refractionColor * hitInfo.dist); float specularChance = hitInfo.material.specularChance; float refractionChance = hitInfo.material.refractionChance; float rayProbability = 1.0f; if (specularChance &gt; 0.0f) { specularChance = FresnelReflectAmount ( hitInfo.fromInside ? hitInfo.material.IOR : 1.0, !hitInfo.fromInside ? hitInfo.material.IOR : 1.0, rayDir, hitInfo.normal, hitInfo.material.specularChance, 1.0f ); float chanceMultiplier = (1.0f - specularChance) / (1.0f - hitInfo.material.specularChance); refractionChance *= chanceMultiplier; } float doSpecular = 0.0f; float doRefraction = 0.0f; float raySelectionRoll = RandomFloat01(rngState); //Wether or not to do the specular reflection ray if (specularChance &gt; 0.0f &amp;&amp; raySelectionRoll &lt; specularChance) { doSpecular = 1.0f; rayProbability = specularChance; } else if (refractionChance &gt; 0.0f &amp;&amp; raySelectionRoll &lt; specularChance + refractionChance) { doRefraction = 1.0f; rayProbability = refractionChance; } else { rayProbability = 1.0f - (specularChance + refractionChance); } rayProbability = max(rayProbability, 0.001f); if (doRefraction == 1.0f) rayPos = (rayPos + rayDir * hitInfo.dist) - hitInfo.normal * c_rayPosNormalNudge; else rayPos = (rayPos + rayDir * hitInfo.dist) + hitInfo.normal * c_rayPosNormalNudge; vec3 diffuseRayDir = normalize(hitInfo.normal + RandomUnitVector(rngState)); vec3 specularRayDir = reflect(rayDir, hitInfo.normal); specularRayDir = normalize(mix(specularRayDir, diffuseRayDir, hitInfo.material.specularRoughness * hitInfo.material.specularRoughness)); vec3 refractionRayDir = refract(rayDir, hitInfo.normal, hitInfo.fromInside ? hitInfo.material.IOR : 1.0f / hitInfo.material.IOR); refractionRayDir = normalize(mix(refractionRayDir, normalize(-hitInfo.normal + RandomUnitVector(rngState)), hitInfo.material.refractionRoughness * hitInfo.material.refractionRoughness)); rayDir = mix(diffuseRayDir, specularRayDir, doSpecular); rayDir = mix(rayDir, refractionRayDir, doRefraction); ret += hitInfo.material.emissive * throughput; if (doRefraction == 0.0f) throughput *= mix(hitInfo.material.albedo, hitInfo.material.specularColor, doSpecular); throughput /= rayProbability; { float p = max(throughput.r, max(throughput.g, throughput.b)); if (RandomFloat01(rngState) &gt; p) break; throughput *= 1.0f / p; } } return ret;} 比较值得提及的是在光线刚碰到物体时, 由于我们不知道光线会传播多远, 所以不能立刻计算出吸收的系数. 我们需要等到光线的下一次碰撞, 才能计算出吸收系数. 环绕相机让我们来我们添加一个鼠标移动相机的功能, 把以下代码添加到main()上方: 123456789101112131415161718192021222324252627282930const float c_minCameraAngle = 0.01f;const float c_maxCameraAngle = (c_pi - 0.01f);const vec3 c_cameraAt = vec3(0.0f, 0.0f, 0.0f);const float c_cameraDistance = 20.0f;void GetCameraVectors(out vec3 cameraPos, out vec3 cameraFwd, out vec3 cameraUp, out vec3 cameraRight){ vec2 mouse = iMouse.xy; if (dot(mouse, vec2(1.0f, 1.0f)) == 0.0f) { cameraPos = vec3(0.0f, 0.0f, -c_cameraDistance); cameraFwd = vec3(0.0f, 0.0f, 1.0f); cameraUp = vec3(0.0f, 1.0f, 0.0f); cameraRight = vec3(1.0f, 0.0f, 0.0f); return; } float angleX = -mouse.x * 16.0f / float(iResolution.x); float angleY = mix(c_minCameraAngle, c_maxCameraAngle, mouse.y / float(iResolution)); cameraPos.x = sin(angleX) * sin(angleY) * c_cameraDistance; cameraPos.y = -cos(angleY) * c_cameraDistance; cameraPos.z = cos(angleX) * sin(angleY) * c_cameraDistance; cameraPos += c_cameraAt; cameraFwd = normalize(c_cameraAt - cameraPos); cameraRight = normalize(cross(vec3(0.0f, 1.0f, 0.0f), cameraFwd)); cameraUp = normalize(cross(cameraFwd, cameraRight));} 再修改一下主函数: 12345678910111213141516171819202122232425262728293031323334void main(){ uint rngState = uint(uint(gl_FragCoord.x) * uint(1973) + uint(gl_FragCoord.y) * uint(9277) + uint(iFrame) * uint(26699)) | uint(1); vec3 cameraPos, cameraFwd, cameraUp, cameraRight; GetCameraVectors(cameraPos, cameraFwd, cameraUp, cameraRight); vec2 jitter = vec2(RandomFloat01(rngState), RandomFloat01(rngState)) - 0.5f; vec3 rayDir; { vec2 uvJittered = (gl_FragCoord.xy + jitter) / iResolution.xy; vec2 screen = uvJittered * 2.0f - 1.0f; float aspectRatio = iResolution.x / iResolution.y; screen.y /= aspectRatio; float cameraDistance = tan(c_FOVDegrees * 0.5f * c_pi / 180.0f); rayDir = vec3(screen, cameraDistance); rayDir = normalize(mat3(cameraRight, cameraUp, cameraFwd) * rayDir); } vec3 color = vec3(0.0f, 0.0f, 0.0f); for (int i = 0; i &lt; c_numRendersPerFrame; ++i) color += GetColorForRay(cameraPos, rayDir, rngState) / float(c_numRendersPerFrame); vec4 lastFrameColor = texture(iChannel0, gl_FragCoord.xy / iResolution.xy); float blend = (iFrame &lt; 2 || iMouse.z &gt; 0.0 || lastFrameColor.a == 0.0f || isKeyPressed(32)) ? 1.0f : 1.0f / (1.0f + (1.0f / lastFrameColor.a)); color = mix(lastFrameColor.rgb, color, blend); gl_FragColor = vec4(color, blend);} 好的, 到此就大功告成了!!! (我实在写不动了…) BTW, 没想到Bandcamp也有Embed功能, 来试试吧 EL MAJA by ROBIN JONES SEVEN","link":"/2020/08/20/Shadertoy_vol4/"},{"title":"My first post","text":"为什么爷要写博客？ 说真的，为什么呢？我也不知道 估计过不了多久就不写了，毕竟也没什么可写的 2020.7.8","link":"/2020/07/08/My_first_post/"},{"title":"Shadertoy学习 vol2. 相机，漫反射，光源","text":"写在开头这篇教程大量引用了blog.demofox.org的内容，但对其中的部分代码做出了修改，以便在vscode的Shader Toy中运行。本文会对部分实现原理做出更加详细的解释，以方便那些对光线追踪和渲染基础没有足够基础的同志。 原博文的链接在此，有需要的可以去参考。 我的代码也可以拿走下载 点击下载 光追的基本原理 光追的基础便是从摄像机处向每个像素发出射线，并检测射线与物体发生的碰撞。当射线碰到光源时，像素便会被照亮。当然细节要比这复杂很多，但你现在已经对原理有了一个基本的认知了，下面就让我们开始一步一步地搭建光追渲染器吧。 创建光线对于每一个屏幕上的像素，都会有一条光线从摄像机穿过它，再射向屏幕内部。所以我们的第一步工作便是确定每一条光线的起点和方向。 我们把像素所在的平面的上方向和右方向设为x轴正方向和y轴正方向，摄像机到屏幕的垂直方向则是z轴正方向。(左手坐标系) 在使用glsl写shader时，经常会涉及到的一个概念便是归一化(Normalize)，比如说在使用iResolution获取屏幕像素坐标时(假设1080p)，我们并不希望右下角的坐标为(1920, 1080)，而更希望是(1, 1)，代表最边角上的像素。把这个大的区间映射到一个统一的区间便是归一化。 对于我们的渲染器，我们会让屏幕的像素坐标处于-1到1这个区间(x轴和y轴都是)，所以我们会用(gl_FragCoord/iResolution.xy) * 2.0f - 1来进行归一化，想象一下函数的位移和缩放。 我们将相机的坐标放在原点，把目标像素所在平面放在z轴上一单位距离远，这样我们就可以写出代码: 12345678910111213141516void main(){ // The ray starts at the camera position (the origin) vec3 rayPosition = vec3(0.0f, 0.0f, 0.0f); // calculate coordinates of the ray target on the imaginary pixel plane. // -1 to +1 on x,y axis. 1 unit away on the z axis vec3 rayTarget = vec3((gl_FragCoord.xy/iResolution.xy) * 2.0f - 1.0f,, 1.0f); // calculate a normalized vector for the ray direction. // it's pointing from the ray position to the ray target. vec3 rayDir = normalize(rayTarget - rayPosition); // show the ray direction gl_FragColor = vec4(rayDir, 1.0f);} 渲染几何体在渲染几何体时，我们需要一个函数，可以在给出几何体位置，形状信息和射线方向位置时返回是否碰撞和碰撞距离。 让我们来定义一个碰撞信息的结构体： 1234567struct SRayHitInfo{ float dist; vec3 normal; vec3 albedo; vec3 emissive;}; 其中albedo和emissive是后面路径追踪时用到的，在这一步还不用刻意理解。 显然，对于不同的几何体，他们的碰撞函数并不相同。在这里，我们会使用TestSphereTrace()和TestQuadTrace()。这两个函数会定义几何体的位置和形状大小，再根据射线位置以及方向来判断碰撞。这两个碰撞函数的推导比较复杂，这里只会提供代码，有兴趣的同学可以自行研究原理。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126float ScalarTriple(vec3 u, vec3 v, vec3 w){ return dot(cross(u, v), w);}bool TestQuadTrace(in vec3 rayPos, in vec3 rayDir, inout SRayHitInfo info, in vec3 a, in vec3 b, in vec3 c, in vec3 d){ // calculate normal and flip vertices order if needed vec3 normal = normalize(cross(c-a, c-b)); if (dot(normal, rayDir) &gt; 0.0f) { normal *= -1.0f; vec3 temp = d; d = a; a = temp; temp = b; b = c; c = temp; } vec3 p = rayPos; vec3 q = rayPos + rayDir; vec3 pq = q - p; vec3 pa = a - p; vec3 pb = b - p; vec3 pc = c - p; // determine which triangle to test against by testing against diagonal first vec3 m = cross(pc, pq); float v = dot(pa, m); vec3 intersectPos; if (v &gt;= 0.0f) { // test against triangle a,b,c float u = -dot(pb, m); if (u &lt; 0.0f) return false; float w = ScalarTriple(pq, pb, pa); if (w &lt; 0.0f) return false; float denom = 1.0f / (u+v+w); u*=denom; v*=denom; w*=denom; intersectPos = u*a+v*b+w*c; } else { vec3 pd = d - p; float u = dot(pd, m); if (u &lt; 0.0f) return false; float w = ScalarTriple(pq, pa, pd); if (w &lt; 0.0f) return false; v = -v; float denom = 1.0f / (u+v+w); u*=denom; v*=denom; w*=denom; intersectPos = u*a+v*d+w*c; } float dist; if (abs(rayDir.x) &gt; 0.1f) { dist = (intersectPos.x - rayPos.x) / rayDir.x; } else if (abs(rayDir.y) &gt; 0.1f) { dist = (intersectPos.y - rayPos.y) / rayDir.y; } else { dist = (intersectPos.z - rayPos.z) / rayDir.z; } if (dist &gt; c_minimumRayHitTime &amp;&amp; dist &lt; info.dist) { info.dist = dist; info.normal = normal; return true; } return false;}bool TestSphereTrace(in vec3 rayPos, in vec3 rayDir, inout SRayHitInfo info, in vec4 sphere){ //get the vector from the center of this sphere to where the ray begins. vec3 m = rayPos - sphere.xyz; //get the dot product of the above vector and the ray's vector float b = dot(m, rayDir); float c = dot(m, m) - sphere.w * sphere.w; //exit if r's origin outside s (c &gt; 0) and r pointing away from s (b &gt; 0) if(c &gt; 0.0 &amp;&amp; b &gt; 0.0) return false; //calculate discriminant float discr = b * b - c; //a negative discriminant corresponds to ray missing sphere if(discr &lt; 0.0) return false; //ray now found to intersect sphere, compute smallest t value of intersection bool fromInside = false; float dist = -b - sqrt(discr); if (dist &lt; 0.0f) { fromInside = true; dist = -b + sqrt(discr); } if (dist &gt; c_minimumRayHitTime &amp;&amp; dist &lt; info.dist) { info.dist = dist; info.normal = normalize((rayPos+rayDir*dist) - sphere.xyz) * (fromInside ? -1.0f : 1.0f); return true; } return false;} 在计算光线是否与物体发生碰撞了之后，我们需要根据结果来改变光线的颜色值。但在这一步我们可以先来验证一下我们是否成功的检测了光线的碰撞，所以先写出一个碰撞后直接返回物体颜色，不进行其他计算的函数。 12345678910111213141516171819202122232425262728293031323334353637const float c_superFar = 10000.0f;vec3 GetColorForRay(in vec3 rayPos, in vec3 rayDir){ SRayHitInfo hitInfo; hitInfo.dist = c_superFar; vec3 ret = vec3(0.0f, 0.0f, 0.0f); if (TestSphereTrace(rayPos, rayDir, hitInfo, vec4(-10.0f, 0.0f, 20.0f, 1.0f))) { ret = vec3(1.0f, 0.1f, 0.1f); } if (TestSphereTrace(rayPos, rayDir, hitInfo, vec4(0.0f, 0.0f, 20.0f, 1.0f))) { ret = vec3(0.1f, 1.0f, 0.1f); } { vec3 A = vec3(-15.0f, -15.0f, 22.0f); vec3 B = vec3( 15.0f, -15.0f, 22.0f); vec3 C = vec3( 15.0f, 15.0f, 22.0f); vec3 D = vec3(-15.0f, 15.0f, 22.0f); if (TestQuadTrace(rayPos, rayDir, hitInfo, A, B, C, D)) { ret = vec3(0.7f, 0.7f, 0.7f); } } if (TestSphereTrace(rayPos, rayDir, hitInfo, vec4(10.0f, 0.0f, 20.0f, 1.0f))) { ret = vec3(0.1f, 0.1f, 1.0f); } return ret;} 看一下结果: 这里我的背景是红的是因为我当时不小心把ret的初始值设成vec3 (1.0f, 0.0f, 0.0f)了，你得到的背景应该还是黑色的。 比例矫正现在出现了一个问题，因为我们的像素所在坐标位置被我们归一化了，所以渲染出来的图形的比例发生了错误，长方形成正方形了。 这里我们在mian()函数里添加两行代码： 123// correct for aspect ratiofloat aspectRatio = iResolution.x / iResolution.y;rayTarget.y /= aspectRatio; 先根据iResolution算出屏幕比例，再改变像素的坐标。 FOV我们的相机距离像素平面为1个单位，经过简单的三角型计算便可以得到我们的视角为90°。如果我们减小相机与像素平面的距离，那FOV便会变大，反之同理。 我们可以修改以下代码来通过计算相机与平面距离来调整相机FOV： 12345678910111213141516171819202122232425262728const float c_FOVDegrees = 90.0f;void main(){ // The ray starts at the camera position (the origin) vec3 rayPosition = vec3(0.0f, 0.0f, 0.0f); // calculate the camera distance float cameraDistance = 1.0f / tan(c_FOVDegrees * 0.5f * c_pi / 180.0f); // calculate coordinates of the ray target on the imaginary pixel plane. // -1 to +1 on x,y axis. 1 unit away on the z axis vec3 rayTarget = vec3((gl_FragCoord.xy/iResolution.xy) * 2.0f - 1.0f, cameraDistance); // correct for aspect ratio float aspectRatio = iResolution.x / iResolution.y; rayTarget.y /= aspectRatio; // calculate a normalized vector for the ray direction. // it's pointing from the ray position to the ray target. vec3 rayDir = normalize(rayTarget - rayPosition); // raytrace for this pixel vec3 color = GetColorForRay(rayPosition, rayDir); // show the result gl_FragColor = vec4(color, 1.0f);} 当FOV设置到120的时候，图形大概是这样的： 最激动人心的一步，光线追踪1.之前我们在碰撞信息里定义了两个变量： 12vec3 emissive //物体自发光的颜色vec3 albedo //在白光下的颜色 对于普通不发光物体，它的emissive会是0向量，albedo里的x, y, z会描述它的RGB颜色；对于光源，它的albedo会是0向量，emissive的x, y, z则描述它的发光颜色。 在这里的光线追踪并不模拟物理世界的法则(虽然我也很想做PBR，不过对我来说太难了)，我们会简单地将像素默认颜色设为黑色，以及一个白色的throughout，然后定义以下规则： 当光线照射到物体上时，emissive * throughout 将添加到像素的颜色上。 当光线照射到物体上时，throughout 会乘以该物体的 albedo，这会影响接下来的光的颜色。 当光线照射到物体上时，将在随机方向上反射并继续与场景相交 当光线错过所有对象，或者到达 N 次反弹时，将终止。(本程序将N设置为了8，若性能需求太高，可自行调整) 举一个例子：当光线击中白球，反弹又击中红球，再次反弹并击中白光，此时像素应是红色。 也就是说，当光线击中有颜色的物体时，接下来的光线都会乘以该物体的颜色。 2.针对反射，我们这里的反射均为漫反射，在宏观上是不规则的。 为此，我们需要一个随机数生成器来获得反射后的光线方向。我们使用像素位置和当前帧数作为随机种子，让每个像素在每帧都能获得不同的随机数 12// initialize a random number state based on frag coord and frameuint rngState = uint(uint(gl_FragCoord.x) * uint(1973) + uint(gl_FragCoord.y) * uint(9277) + uint(iFrame) * uint(26699)) | uint(1); 把它放置在main()函数内部，再添加其他功能 123456789101112131415161718192021222324uint wang_hash(inout uint seed){ seed = uint(seed ^ uint(61)) ^ uint(seed &gt;&gt; uint(16)); seed *= uint(9); seed = seed ^ (seed &gt;&gt; 4); seed *= uint(0x27d4eb2d); seed = seed ^ (seed &gt;&gt; 15); return seed;} float RandomFloat01(inout uint state){ return float(wang_hash(state)) / 4294967296.0;} vec3 RandomUnitVector(inout uint state){ float z = RandomFloat01(state) * 2.0f - 1.0f; float a = RandomFloat01(state) * c_twopi; float r = sqrt(1.0f - z * z); float x = r * cos(a); float y = r * sin(a); return vec3(x, y, z);} 把之前GetColorForRay()的内容放进TestSceneTrace()内。这时，我们可以完成真正的GetColorForRay()了: 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576void TestSceneTrace(in vec3 rayPos, in vec3 rayDir, inout SRayHitInfo hitInfo){ { vec3 A = vec3(-15.0f, -15.0f, 22.0f); vec3 B = vec3( 15.0f, -15.0f, 22.0f); vec3 C = vec3( 15.0f, 15.0f, 22.0f); vec3 D = vec3(-15.0f, 15.0f, 22.0f); if (TestQuadTrace(rayPos, rayDir, hitInfo, A, B, C, D)) { hitInfo.albedo = vec3(0.7f, 0.7f, 0.7f); hitInfo.emissive = vec3(0.0f, 0.0f, 0.0f); } } if (TestSphereTrace(rayPos, rayDir, hitInfo, vec4(-10.0f, 0.0f, 20.0f, 1.0f))) { hitInfo.albedo = vec3(1.0f, 0.1f, 0.1f); hitInfo.emissive = vec3(0.0f, 0.0f, 0.0f); } if (TestSphereTrace(rayPos, rayDir, hitInfo, vec4(0.0f, 0.0f, 20.0f, 1.0f))) { hitInfo.albedo = vec3(0.1f, 1.0f, 0.1f); hitInfo.emissive = vec3(0.0f, 0.0f, 0.0f); } if (TestSphereTrace(rayPos, rayDir, hitInfo, vec4(10.0f, 0.0f, 20.0f, 1.0f))) { hitInfo.albedo = vec3(0.1f, 0.1f, 1.0f); hitInfo.emissive = vec3(0.0f, 0.0f, 0.0f); } if (TestSphereTrace(rayPos, rayDir, hitInfo, vec4(10.0f, 10.0f, 20.0f, 5.0f))) { hitInfo.albedo = vec3(0.0f, 0.0f, 0.0f); hitInfo.emissive = vec3(1.0f, 0.9f, 0.7f) * 100.0f; } }vec3 GetColorForRay(in vec3 startRayPos, in vec3 startRayDir, inout uint rngState){ // initialize vec3 ret = vec3(0.0f, 0.0f, 0.0f); vec3 throughput = vec3(1.0f, 1.0f, 1.0f); vec3 rayPos = startRayPos; vec3 rayDir = startRayDir; for (int bounceIndex = 0; bounceIndex &lt;= c_numBounces; ++bounceIndex) { // shoot a ray out into the world SRayHitInfo hitInfo; hitInfo.dist = c_superFar; TestSceneTrace(rayPos, rayDir, hitInfo); // if the ray missed, we are done if (hitInfo.dist == c_superFar) break; // update the ray position rayPos = (rayPos + rayDir * hitInfo.dist) + hitInfo.normal * c_rayPosNormalNudge; // calculate new ray direction, in a cosine weighted hemisphere oriented at normal rayDir = normalize(hitInfo.normal + RandomUnitVector(rngState)); // add in emissive lighting ret += hitInfo.emissive * throughput; // update the colorMultiplier throughput *= hitInfo.albedo; } // return pixel color return ret;} 这里我们给一开始的光线碰撞距离设置了一个极大值，如果反射8次了碰撞距离还未更新，则认为光线未发生碰撞。这里用cosine weight hemisphere是因为Lambert cosine rule, 能给出更真实的反射效果，追求深刻理解的可以去自行谷歌。 运行一下，看看效果: 这时的图像只是一堆零散的点是因为每一帧里光线打到的点都不一样，我们需要让像素去显示在所有帧里的平均值。 平均像素值我们可以添加多个通道来平均像素值。这里新建一个文件，把前一个文件选为iChannel0: 1234567#iChannel0 \"file://test3.frag\"void main(){ vec3 color = texture(iChannel0, gl_FragCoord.xy / iResolution.xy).rgb; gl_FragColor = vec4(color, 1.0f); 同时在上一个文件中添加自己作为一个channel: 1#iChannel0 \"self\" 再往main()中添加混合像素值的代码: 12vec3 lastFrameColor = texture(iChannel0, gl_FragCoord.xy / iResolution.xy).rgb;color = mix(lastFrameColor, color, 1.0f / float(iFrame + 1)); 最后我们可以再布置一下场景： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102void TestSceneTrace(in vec3 rayPos, in vec3 rayDir, inout SRayHitInfo hitInfo){ // to move the scene around, since we can't move the camera yet vec3 sceneTranslation = vec3(0.0f, 0.0f, 10.0f); vec4 sceneTranslation4 = vec4(sceneTranslation, 0.0f); // back wall { vec3 A = vec3(-12.6f, -12.6f, 25.0f) + sceneTranslation; vec3 B = vec3( 12.6f, -12.6f, 25.0f) + sceneTranslation; vec3 C = vec3( 12.6f, 12.6f, 25.0f) + sceneTranslation; vec3 D = vec3(-12.6f, 12.6f, 25.0f) + sceneTranslation; if (TestQuadTrace(rayPos, rayDir, hitInfo, A, B, C, D)) { hitInfo.albedo = vec3(0.7f, 0.7f, 0.7f); hitInfo.emissive = vec3(0.0f, 0.0f, 0.0f); } } // floor { vec3 A = vec3(-12.6f, -12.45f, 25.0f) + sceneTranslation; vec3 B = vec3( 12.6f, -12.45f, 25.0f) + sceneTranslation; vec3 C = vec3( 12.6f, -12.45f, 15.0f) + sceneTranslation; vec3 D = vec3(-12.6f, -12.45f, 15.0f) + sceneTranslation; if (TestQuadTrace(rayPos, rayDir, hitInfo, A, B, C, D)) { hitInfo.albedo = vec3(0.7f, 0.7f, 0.7f); hitInfo.emissive = vec3(0.0f, 0.0f, 0.0f); } } // cieling { vec3 A = vec3(-12.6f, 12.5f, 25.0f) + sceneTranslation; vec3 B = vec3( 12.6f, 12.5f, 25.0f) + sceneTranslation; vec3 C = vec3( 12.6f, 12.5f, 15.0f) + sceneTranslation; vec3 D = vec3(-12.6f, 12.5f, 15.0f) + sceneTranslation; if (TestQuadTrace(rayPos, rayDir, hitInfo, A, B, C, D)) { hitInfo.albedo = vec3(0.7f, 0.7f, 0.7f); hitInfo.emissive = vec3(0.0f, 0.0f, 0.0f); } } // left wall { vec3 A = vec3(-12.5f, -12.6f, 25.0f) + sceneTranslation; vec3 B = vec3(-12.5f, -12.6f, 15.0f) + sceneTranslation; vec3 C = vec3(-12.5f, 12.6f, 15.0f) + sceneTranslation; vec3 D = vec3(-12.5f, 12.6f, 25.0f) + sceneTranslation; if (TestQuadTrace(rayPos, rayDir, hitInfo, A, B, C, D)) { hitInfo.albedo = vec3(0.7f, 0.1f, 0.1f); hitInfo.emissive = vec3(0.0f, 0.0f, 0.0f); } } // right wall { vec3 A = vec3( 12.5f, -12.6f, 25.0f) + sceneTranslation; vec3 B = vec3( 12.5f, -12.6f, 15.0f) + sceneTranslation; vec3 C = vec3( 12.5f, 12.6f, 15.0f) + sceneTranslation; vec3 D = vec3( 12.5f, 12.6f, 25.0f) + sceneTranslation; if (TestQuadTrace(rayPos, rayDir, hitInfo, A, B, C, D)) { hitInfo.albedo = vec3(0.1f, 0.7f, 0.1f); hitInfo.emissive = vec3(0.0f, 0.0f, 0.0f); } } // light { vec3 A = vec3(-5.0f, 12.4f, 22.5f) + sceneTranslation; vec3 B = vec3( 5.0f, 12.4f, 22.5f) + sceneTranslation; vec3 C = vec3( 5.0f, 12.4f, 17.5f) + sceneTranslation; vec3 D = vec3(-5.0f, 12.4f, 17.5f) + sceneTranslation; if (TestQuadTrace(rayPos, rayDir, hitInfo, A, B, C, D)) { hitInfo.albedo = vec3(0.0f, 0.0f, 0.0f); hitInfo.emissive = vec3(1.0f, 0.9f, 0.7f) * 20.0f; } } if (TestSphereTrace(rayPos, rayDir, hitInfo, vec4(-9.0f, -9.1f, 18.0f, 3.0f)+sceneTranslation4)) { hitInfo.albedo = vec3(0.9f, 0.9f, 0.75f); hitInfo.emissive = vec3(0.0f, 0.0f, 0.0f); } if (TestSphereTrace(rayPos, rayDir, hitInfo, vec4(0.0f, -9.1f, 23.0f, 3.0f)+sceneTranslation4)) { hitInfo.albedo = vec3(0.9f, 0.75f, 0.9f); hitInfo.emissive = vec3(0.0f, 0.0f, 0.0f); } if (TestSphereTrace(rayPos, rayDir, hitInfo, vec4(5.5f, -9.1f, 20.0f, 3.0f)+sceneTranslation4)) { hitInfo.albedo = vec3(0.75f, 0.9f, 0.9f); hitInfo.emissive = vec3(0.0f, 0.0f, 0.0f); } } 定义了5个墙面，3个球体和一个灯光，组成了图形学的Hello World: Cornell box 让我们的渲染结果看起来更nb一点: 这里我的球并没有贴地放置，因为不知道为什么会出现奇怪的反光，虽然肯定跟没有环境光遮蔽无关，不过还得让我再研究研究。 End下一期的教程也会根据这个框架来做更深入的效果，敬请期待。","link":"/2020/08/07/Shadertoy_vol2/"},{"title":"DIY IIDX Controller","text":"1. IntroThis repo includes CAD files and arduino scripts to help you build your own IIDX controller with relatively low budget. You can access my repository at my Github page: IIDX-DIY For the case of the controllerThe case is build with acrylic. You will need to find a factory and provide them with the CAD files. After getting them all cutted, you can glue them together. Some parts will also need screws to be assembled, you need to buy them in stores. For the circuit of the controllerWe are using arduino as the basic control device here. For my own controller, I am using the arduino micro, but of course you can choose other type of arduino, but to ensure they have enough ports. For building the complete circuit, the basic tools you will need are wires, soldering tin, and electric soldering iron. Besides, the bottons for the controller, the connection cables will also be needed. Since you could hardly find an excactly same type of the botton with the IIDX arcade, I will recommend you to search arcade botton on AliExpress. The encoder is also needed for the rotating part. 2. Let’s begin!!!Building the caseAfter you have get the cutted acrylic, you can assemble them according to the diagram below. notice that there are special glue for acrylic, it can dissolve the acrylic so the case would be in better quality. The bottom plank of the case should not be glued, be careful. The extra hole on the top panel of the box is for moving the disk futher to attain the arcade distance standard. And for you can moving the disk anytime you want, you should use metal columns and glue to assemble the support for the disk. Building the circuitThe circuit part is based on arduino. 1 encoder, 9 bottons, wires, and a long microUSB cable are needed. Though you can use different model of arduino, I recommend you to use the arduino micro, since it could be powered directly by the microUSB port. For the wiring, I soldered my arduino on a board with holes and use wire to prolong the position of connecting the bottons. I used port 4, 5, 7, 8, 9, 10, 14, 15 and 16 on arduino for connecting 9 bottons. The encoder used port 2 and 3 It’s necessary for encoder to use the 2 and 3 ports for special function. Do not change them! And you will have to give all the bottons a common ground, which is GND port on arduino and give encoder a 5V and GND. My controller’s circuit looks like this: Now for the programsThe program will analog a keyboard input. You should have seen my program file in the repo, download an IDE for arduino and just upload that program. If you are using different port or key configuration, remember to change the buttonFunc function. Change this section to give different ports with different keys: buttonFunc(6, &apos;g&apos;); buttonFunc(7, &apos;h&apos;); buttonFunc(8, &apos;j&apos;); buttonFunc(9, &apos;v&apos;); buttonFunc(10, &apos;b&apos;); buttonFunc(14, &apos;n&apos;); buttonFunc(16, &apos;m&apos;); buttonFunc(4, &apos;q&apos;); buttonFunc(15, &apos;e&apos;);One more thing to notice is that the encoder I used is 600 p/r, if you are using different encoder, the sensitivity would change, and you can adjust that by changing the the first parameter in the encoderFunc. encoderFunc(3, KEY_RIGHT_SHIFT, KEY_LEFT_SHIFT); //change the 3 to a larger value for higher sensitivity //Vice versa3. Something elseThe rotate disk of my controller is not fixed to the encoder, so it might drop. If you want to solve that problem, you could change the cad file of the transparent one. Replacing the disk with the part under it. Then you could print this part with the 3D model included in my repo and glue them together. Since 3D printing is in better accuracy, it could be tightly attached to the encoder. Have fun in IIDX! vosaica","link":"/2020/07/12/IIDX_DIY_Controller/"},{"title":"Shadertoy学习 vol5.5. Ray Marching 番外篇","text":"不好意思，最近申请太忙了，没什么时间更新，只好拿一个我的课堂小作业来水。基本内容还是跟上次的Ray Marching一样，不过这次拿python来写一下，把图像拿ASCII字符渲染打到控制台去。 代码在此 这个作业不让import外部包，所以有一些地方得自己实现基础功能了。 线性代数计算123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172class Vector(): def __init__(self, array): self.array = array self.row = len(array) self.column = -1 def __str__(self): c = \"[\" for i in range(self.row): if i == self.row - 1: c += \" \" + str(self.array[i]) elif i == 0: c += str(self.array[i]) else: c += \" \" + str(self.array[i]) c += \"]\" return c def __add__(self, other): c = Vector([0 for i in range(self.row)]) for i in range(0, self.row): c.array[i] = self.array[i] + other.array[i] return c def __sub__(self, other): c = Vector([0 for i in range(self.row)]) for i in range(0, self.row): c.array[i] = self.array[i] - other.array[i] return c def __mul__(self, other): c = Vector([0 for i in range(self.row)]) for i in range(0, self.row): c.array[i] = self.array[i] * other return c def __truediv__(self, other): c = Vector([0 for i in range(self.row)]) for i in range(0, self.row): c.array[i] = self.array[i] / other return c def divVec(self, other): c = Vector([0 for i in range(self.row)]) for i in range(0, self.row): c.array[i] = self.array[i] / other.array[i] return c def innerProd(self, other): c = 0 for i in range(0, self.row): c += self.array[i] * other.array[i] return c def outerProd(self, other): c = Matrix([[0 for j in range(self.row)] for i in range(self.row)]) for i in range(0, self.row): for j in range(0, self.row): c.array[i][j] = self.array[i] * other.array[j] return c def length(self): if (len(self.array) == 3): return (self.array[0]**2 + self.array[1]**2 + self.array[2]**2)**0.5 def normalize(self): c = Vector([0 for i in range(self.row)]) norm = self.length() for i in range(0, self.row): c.array[i] = self.array[i] / norm return cclass Matrix(): def __init__(self, array): self.array = array self.row = len(array) self.column = len(array[0]) def __str__(self): maxlen = [0 for j in range(self.column)] for i in range(0, self.row): for j in range(0, self.column): if len(str(self.array[i][j])) &gt; maxlen[j]: maxlen[j] = len(str(self.array[i][j])) a = \"[\" for i in range(0, self.row): if i == 0: a += \"[\" else: a += \" [\" for j in range(0, self.column): extraspace = maxlen[j] - len(str(self.array[i][j])) if j == self.column - 1: a += \" \" * extraspace + str(self.array[i][j]) elif j == 0: a += \" \" + \" \" * extraspace + str(self.array[i][j]) + \" \" else: a += \" \" * extraspace + str(self.array[i][j]) + \" \" if i == self.row - 1: a += ']' else: a += ']' + \"\\n\" a += ']' return a def __add__(self, other): c = Matrix([[0 for j in range(self.column)] for i in range(self.row)]) for i in range(0, self.row): for j in range(0, self.column): c.array[i][j] = self.array[i][j] + other.array[i][j] return c def __sub__(self, other): c = Matrix([[0 for j in range(self.column)] for i in range(self.row)]) for i in range(0, self.row): for j in range(0, self.column): c.array[i][j] = self.array[i][j] - other.array[i][j] return c def __mul__(self, other): if other.column == -1: c = Vector([1 for i in range(self.row)]) for i in range(0, self.row): c.array[i] = self.__mulVec(other, i) return c else: c = Matrix([[0 for j in range(other.column)] for i in range(self.row)]) for i in range(0, c.row): for j in range(0, c.column): c.array[i][j] = self.__mulEle(other, i, j) return c def __mulEle(self, other, i, j): ele = 0 for k in range(0, self.column): ele += self.array[i][k] * other.array[k][j] return ele def __mulVec(self, other, i): ele = 0 for k in range(0, self.column): ele += self.array[i][k] * other.array[k] return ele def eleWise(self, other): c = Matrix([[0 for j in range(self.column)] for i in range(self.row)]) for i in range(0, self.row): for j in range(0, self.column): c.array[i][j] = self.array[i][j] * other.array[i][j] return c''' Examplesa = Matrix([[1, 2, 5], [5, 0, 2], [1, 3, 4]])b = Matrix([[3, 4, 1], [0, 2, 1], [2, 3, 7]])c = Vector([2, 3, 1])d = Vector([2, 4, 2])print(\"-----\" * 20)print(a + b, \"\\n\")print(a - b, \"\\n\")print(a * b, \"\\n\")print(a * c, \"\\n\")print(Matrix.eleWise(a, b), \"\\n\")print(Vector.innerProd(c, d), \"\\n\")print(Vector.outerProd(c, d), \"\\n\")''' 写了两个类，一个Vector, 一个Matrix。这部分没什么值得提的，有需要可以换成numpy来算。不过有很多地方就得改。 屏幕这个程序还是由CPU计算的，一方面是因为我没有什么GPU方面的编程经验，另一方面则是这个程序还是不让引用外部包。 1234567891011121314151617class Screen(): def __init__(self, width, height): self.width = width self.height = height self.fragCoord = Matrix( [[Vector([j + 0.5, i + 0.5]) for j in range(width)] for i in range(height - 1, -1, -1)]) self.resolution = Vector([self.width, self.height]) self.fragColor = Matrix([[Vector([0]) for j in range(width)] for i in range(height)]) self.uv = self.fragCoord for i in range(height): for j in range(width): self.uv.array[i][j] = Vector.divVec( self.fragCoord.array[i][j] - self.resolution * 0.5, self.resolution) 这里我设置了两个二维数组，fragcoord里面存放的是归一化了的像素坐标，fragcoolor则存放之后计算好的像素值 RayMarch这部分没有任何变化，只是把之前的GLSL用python重写了一遍。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657SURFACE_DIST = 0.01MAX_DIST = 150.0MAX_STEP = 100def clamp(x, low, high): return min(max(x, low), high)def getDist(p): s = Vector([0, 1, 6]) sRadius = 1.0 sphereDist = Vector.length(p - s) - sRadius planeDist = p.array[1] dist = min(sphereDist, planeDist) return distdef rayMarch(ro, rd): depth = 0 for _ in range(MAX_STEP): pos = ro + rd * depth dist = getDist(pos) depth += dist if dist &lt; SURFACE_DIST or depth &gt; MAX_DIST: break return depthdef getNormal(p): dx = Vector([0.01, 0.0, 0.0]) dy = Vector([0.0, 0.01, 0.0]) dz = Vector([0.0, 0.0, 0.01]) dist = getDist(p) n = Vector([ dist - getDist(p - dx), dist - getDist(p - dy), dist - getDist(p - dz) ]) return (n.normalize())def getLight(p): lightPos = Vector([3, 5, -1]) li = Vector.normalize(lightPos - p) n = getNormal(p) dif = clamp(Vector.innerProd(n, li), 0, 1) if p.array[1] &lt; 0.1: dif *= float(int(p.array[0] + 100) % 2 ^ int(p.array[2] + 100) % 2) * 0.9 d = rayMarch(p + n * (SURFACE_DIST * 2), li) if d &lt; Vector.length(lightPos - p): dif *= 0.1 return dif 字符转化因为要打到控制台去，并没有GUI，所以我们会根据灰度值来对应字符。我直接去网上找了个别人列好的ASCII字符表，然后只要按顺序把fragcoolor里的浮点数转换成字符就好了。 12345678class Screen(): def turn2Chara(self, number): charas = list('''@B%8&amp;WM#*oahkbdpqwmZO0QLCJUYXzcvunxr\\ jft/\\\\|()1{}[]?-_+~&lt;&gt;i!lI;:,\"^`'. ''') charas_len = len(charas) chara = charas[int(number * charas_len)] return chara 当然这里存在一个问题，我们直接把线性色彩给搞成灰度值了。照理来说我们会先把线性色彩给映射到SRGB空间，然后再合并通道的，直接换成灰色的会让一些微小的颜色差异消失，尤其当我们只有这几个ASCII字符时。不过这就是个简单的小作业，所以我懒得整了。有兴趣的可以参考 Shadertoy学习 vol3. 抗锯齿，SRGB，镜面反射 里的SRGB那一节。 并行计算虽然这个作业不让引用外部库，但我还是毅然决然地加上了并行计算的部分。我TM直接import multiprocessing 一方面是只用CPU太慢了，另一方面是我想顺便试试python中的并行计算。 12345678910111213141516171819202122232425262728293031323334def calcPart(self, start, end): result = Matrix([[Vector([0]) for j in range(self.width)] for i in range(self.height)]) for i in range(start, end): for j in range(self.width): uv = self.uv.array[i][j] color = Vector([0, 0, 0]) ro = Vector([0, 1, 0]) rd = Vector([uv.array[0], uv.array[1], 1]).normalize() d = rayMarch(ro, rd) p = ro + rd * d dif = getLight(p) color = Vector([dif]) result.array[i][j] = color return result def calcMul(self, processorNum): from multiprocessing import Pool start, interval = 0, int(self.height / processorNum) pool = Pool(processes=processorNum) processes = [] for _ in range(processorNum): processes.append( pool.apply_async(self.calcPart, (start, start + interval))) start += interval pool.close() pool.join() for process in processes: self.fragColor += process.get() 这里为了避免同时对变量进行修改造成问题，我就简单粗暴地生成了多个实例，分别算完后再把结果组合在一起。 地板封面的地板是方砖形的，之前看iq大神的样例时就想给我的也加上，这次没看他的思路，自己想了想，用异或运算写的。大概就是x轴坐标和z轴坐标的整数部分奇偶不同时给颜色值乘上个系数 如果有不用if来判断是否在平面上的方法，请务必在评论区告诉我！ 12if p.array[1] &lt; 0.1: dif *= float(int(p.array[0] + 100) % 2 ^ int(p.array[2] + 100) % 2) * 0.9 结语申请季实在是有点忙，下期一定不水，敬请期待！","link":"/2020/10/20/Shadertoy_vol5.5/"},{"title":"Shadertoy学习 vol5. Ray Marching 基本原理与实现","text":"从今天，我们的Shadertoy之旅才真正开始。这是因为之前的Path Tracing虽然效果很好，但是他的运行实在是太慢了，根本不可能在现在的计算机上实现实时的渲染。光为了一张图，我们就得花上几秒，甚至数十秒，要想做出动画的效果，我们得想想别的渲染方法。 为此，我们采用了一种新的叫做Ray Marching的渲染方法。 Ray Marching基本介绍Ray Marching，中文叫做光线步进，正如他的名字一样，也是基于光线的渲染。他和Ray Tracing或者Path Tracing最大的区别在于求交的方法。在之前的Path Tracing中，我们自己写了求解光线是否与矩形或者球形相交的函数，如果我们用三角形来进行建模则需要大量求解光线是否与三角形相交。然而在光线步进当中，我们不再直接求解光线是否与物体相交，我们现在会使用SDF函数来求光线当前位置与物体的最小距离。如果距离为正，则未发生碰撞，如果距离为负，则光线在物体内部，如果距离为0，则恰好发生碰撞。Ray Marching最大的优势便在于渲染的高效和使用SDF来建模的方便程度。 Sphere Tracing试想我们要开始写一个光线步进的算法，一开始我们可能会想成这样: 但是现在会有一个问题，当我们的物体过小或者步长过大时,光线会直接穿过去: 这样渲染的时候物体就根本不会被显示。如果我们为了不错过物体而把步长设置的过小，计算量又会太大。 为此，我们需要一种能够实时调整步长的算法，既能让我们不会错过物体，又不会让我们的计算开销太大。而Sphere Traching就是这样的一种算法。 我们在起点使用SDF检测最近距离，然后将当前的最近距离设为步长。接下来一直重复这两步，直到最小距离等于0或小于我们设置的一个Threshold。 基础知识都知道了，让我们开始编程实现吧。 SDF获得最近距离我们需要一个函数，输入一个点的坐标，返回离这个点最近的物体的距离。 设点的坐标\\(P\\left ( P_{x}, P_{y}, P_{z} \\right )\\)，球体的圆心为\\(S\\left ( S_{x}, S_{y}, S_{z} \\right )\\)，半径为\\(R\\)。易证：$$d_{\\text {Sphere}}=\\operatorname{length}(P-S)-r$$ 除了球体，我们还设置了一个平面，SDF公式非常简单:$$d_{\\text {Plane}}=P_{y}$$ 代码为这样: 123456789101112float GetDist(vec3 p){ vec4 s = vec4(-1.0, 1, 6, 1.3); float sphereDist = length(p - s.xyz) - s.w; float planeDist = p.y; vec4 s1 = vec4(1.0, 0.5, 6, 0.9); float sphereDist2 = length(p - s1.xyz) - s1.w; float d = min(sphereDist, min(planeDist, sphereDist2)); return d;} 法线这一部分用到了梯度求解法线的方法，设求最近距离$d=f(x, y, z)$，其中$(x, y, z)$为世界坐标，$d$为距离，$f$是我们的GetDist函数。当点$P$位于任意物体表面时，我们可以靠计算梯度得到表面向量$N$:$$N=f_{x}\\left(x_{p}, y_{p}, z_{p}\\right) i+f_{y}\\left(x_{p}, y_{p}, z_{p}\\right) j+f_{z}\\left(x_{p}, y_{p}, z_{p}\\right) k$$其中$i, j, k$为$x, y, z$方向的单位向量。 代码如下: 12345678910111213vec3 GetNormal(vec3 p){ float d = GetDist(p); vec2 e = vec2(0.01f, 0.0f); vec3 n = d - vec3 ( GetDist(p - e.xyy), GetDist(p - e.yxy), GetDist(p - e.yyx) ); return normalize(n);} 计算光线这里用的是最简单的Diffuse Lighting, 也叫兰伯特光照模型: $$\\boldsymbol{c}_{\\text {diffuse}}=\\left(\\boldsymbol{c}_{\\text {light}} \\cdot \\boldsymbol{m}_{\\text {diffuse}}\\right) \\max (0, \\hat{\\boldsymbol{n}} \\cdot \\hat{\\boldsymbol{l}})$$ 这里的$N$就是我们之前靠梯度算出来的法线向量。 12345678910111213float GetLight(vec3 p){ vec3 lightPos = vec3(0, 5, 6); lightPos.xz += vec2(sin(iTime),cos(iTime))*2.0; vec3 l = normalize(lightPos - p); vec3 n = GetNormal(p); float dif = clamp(dot(n, l), 0.0f, 1.0f); float d = RayMarch(p + n * SURF_DIST * 2.0f, l); if (d &lt; length(lightPos - p)) dif*=0.1; return dif;} 10到11行里阴影的实现方法非常有趣，当物体上任意一点$P$向光源RayMarch时得到的距离小于该点到光源的直线距离时，说明该点和光源之间一定存在物体遮挡住了光线。所以我们将这点的颜色值乘以0.1，以表示阴影。 Raymarch部分12345678910111213float RayMarch(vec3 ro, vec3 rd){ float d0 = 0.0f; for (int i = 0; i &lt; MAX_STEPS; i++) { vec3 p = ro + rd * d0; float ds = GetDist(p); d0+=ds; if (d0 &gt; MAX_DIST || ds &lt; SURF_DIST) break; } return d0;} 主函数部分这里没有什么难的部分，我们就是定义了相机和初始光线的方向，然后走了一遍我们的渲染管线，最后给像素颜色附了值。 12345678910111213141516void main(){ vec2 uv = (gl_FragCoord.xy - 0.5f * iResolution.xy) / iResolution.y; vec3 color = vec3(0); vec3 ro = vec3(0, 1, 0); vec3 rd = normalize(vec3(uv.x, uv.y, 1.0f)); float d = RayMarch(ro, rd); vec3 p = ro + rd * d; float dif = GetLight(p); color = vec3(dif); gl_FragColor = vec4(color, 1.0);} 结尾关于Ray Marching的基础介绍就到这里了，下一篇会介绍更多关于SDF函数和过程化建模的知识。再见！","link":"/2020/09/28/Shadertoy_vol5/"},{"title":"Shadertoy学习 vol6. SDF的变换，软阴影和移动相机","text":"SDF的平移旋转和颜色SDF(Signed Distance Function), 顾名思义，是计算距离的函数。之前我们有提到了球体的SDF。但其实还有很多别的形状的SDF。这里推荐大家去看iq大神的博客，他就是建立了Shadertoy这个网站的人。Inigo Quilez 上次我们给RayMarching渲染添加了基础的内容，但是还没有颜色。我们在raymarching这一步里添加上碰撞时的颜色信息，不只返回距离值，还返回最近物体的RGB颜色值。这里图方便我就直接用vec4()了，如果想要更规范一点也可以定义一个结构体。我们添加一个新的函数MinDist()，这里传入的两个vec4是SDF返回的距离和物体的RGB值。RGB值要在这一步处理，要不然碰撞时我们就不知道究竟是碰到什么颜色的物体了。 这里我们的getDist()函数也变了，让我们先把SDF函数写到外面去 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677vec4 MinDist(vec4 a, vec4 b){ return a.x &lt; b.x ? a:b;}mat3x3 rotateY(in float theta){ return mat3x3 ( 1, 0, 0, 0, cos(theta), -sin(theta), 0, sin(theta), cos(theta) );}mat3x3 rotateX(in float theta){ return mat3x3 ( cos(theta), 0, sin(theta), 0, 1, 0, -sin(theta), 0, cos(theta) );}float sdPlane(vec3 p){ return p.y;}float sdSphere( vec3 p, float s ){ return length(p)-s;}float sdBox( vec3 p, vec3 b ){ vec3 q = abs(p) - b; return length(max(q,0.0)) + min(max(q.x,max(q.y,q.z)),0.0);}float sdOctahedron( vec3 p, float s){ p = abs(p); float m = p.x+p.y+p.z-s; vec3 q; if( 3.0*p.x &lt; m ) q = p.xyz; else if( 3.0*p.y &lt; m ) q = p.yzx; else if( 3.0*p.z &lt; m ) q = p.zxy; else return m*0.57735027; float k = clamp(0.5*(q.z-q.y+s),0.0,s); return length(vec3(q.x,q.y-s+k,q.z-k)); }vec4 getDist(vec3 p){ vec4 d = vec4(1e10, 0.0, 0.0, 0.0); mat3x3 rotateX = rotateX(iTime); mat3x3 rotateY = rotateY(iTime); d = minDist(d, vec4(sdSphere(p - vec3(-1, -0.5, 2.0), 0.5), 1.0, 0.83, 0.4)); d = minDist(d, vec4(sdPlane(p), 1.0, 1.0, 1.0)); d = minDist(d, vec4(sdOctahedron(p - vec3(3, cos(iTime), -1.0), 1.0), 0.5, 0.2, 0.6)); //d = minDist(d, vec4(sdBox(p - vec3(1, -0.5, 2), vec3(0.5, 0.5, 0.5)), 0.5, 0.5, 1)); d = minDist(d, vec4(sdBox(rotateX * rotateY * p - rotateX * rotateY * vec3(0.6, 0.5, 2), vec3(0.6, 0.6, 0.6)), 0.5, 0.5, 1)); return d;} 调用sdf函数的时候减去位移向量就可以移动我们的物体，为了旋转我们定义了x轴和y轴的旋转矩阵，不会的可以回家补习线性代数，然后左乘位置向量就是绕着世界坐标系旋转。为了在自身坐标系下旋转，我们需要在变换后再减去位移旋转矩阵左乘位移向量的值。 rotateY(iTime)里的iTime也可以填角度的值，不过我想让它一直转而已。 半兰伯特光照模型我们的getDist()现在返回的是vec4了，所以我们的渲染函数也要改变一下。上次的getLight()被我们修名成render()，不再传进点的位置，而是传进光线起点ro和光线方向rd，这会方便我们在渲染函数里调用我们的软阴影。 123456789101112131415161718192021222324252627282930313233343536373839vec3 render(vec3 ro, vec3 rd, vec2 uv){ vec3 dif; float d = rayMarch(ro, rd); vec3 p = ro + rd * d; if (d &gt; 50.0) { dif = vec3(0.38, 0.57, 0.88) - max(rd.y,0.0)*0.8; //dif = clamp(normalize(vec3(98, 146, 226)) * uv.y + 0.1 * 4.5, 0.0, 1.0); //Sky color return dif; } vec3 lightPos = vec3(2, 15, 2); lightPos.x += cos(iTime) * 8.0; lightPos.z += sin(iTime) * 8.0; vec3 l = normalize(lightPos - p); vec3 c = vec3(1.0); vec3 n = getNormal(p, c); #ifdef HALF_LAMBERT float nDot = 0.5 * dot(n, l) + 0.5; #else float nDot = dot(n, l); #endif dif = vec3(1.0, 1.0, 0.9) * c * clamp(nDot, 0.0, 1.0); #ifdef FLOOR_GRID if (p.y &lt; -0.98 || abs(p.x)&gt; 20.0 || abs(p.z) &gt; 20.0)//floor color { dif -= float((int(p.x+100.0) % 2) ^ (int(p.z+100.0)) % 2) * 0.1; } #endif return dif;} 颜色的渲染直接用光线强度乘上代表颜色的c就好了。我们这里用到了半兰伯特光照模型，解决了光线背面部分过暗的问题。至于为什么是一半，其实无所谓。这只是经验公式，最早是VALVE的工程师提出来的。你当然也可以设计自己的光照模型。 到现在这一步你就应该可以看到物体颜色了。 这里的if (d &gt; 50.0)的条件是为了判断距离太远时渲染天空的，可以根据自己想要的效果适当调整。 软阴影我们之前添加阴影的方法当用作在现在的渲染函数里时，需要传入当前点的位置和光线方向,如果还是之前的逻辑的话，我们可以写出以下代码： 12345678910float shadow(in vec3 ro, in vec3 rd, float mint, float maxt){ for(float t = mint; t &lt; maxt;) { float h = getDist(ro + rd * t).x; if(h &lt;0.001) return 0.4; t += h; } return 1.0;} 我们从物体表面ro出发，向光线方向rd做raymarch,碰到物体返回0.4做阴影强度;没碰到则返回1.0，强度不变。现在多出来的mint和maxt是为了让光线只在一定范围内检测遮挡物体，可以减少运算量，也方便调整效果。 我们现在来加上软阴影效果，也就是让阴影的轮廓变得更加柔和。这能增加我们渲染的真实感，因为现实生活中多光源和漫反射的存在，阴影不一定有明显的轮廓。这里上一张图，说明一下软阴影的基本原理。对于原来的硬阴影，我们依旧保留，并称它为本影区。我们在本影区的外围再加上一层半影区。之前我们对于本影区会严格要求光线只有被遮挡后才会产生阴影，但对于半影区我们可以不用了。 我们还是从物体表面ro出发，向光线方向rd做raymarch，然后设任意一点到物体的最短距离为d，光线走过的距离为t。那么根据常识来考虑，这一点的半影强度应该与t成正比，与d成反比。最近距离(d)越小，阴影越强。最近距离发生距离(t)越大，阴影越强。t这里我再解释一下，因为阴影是越靠进物体越强，越远离物体越弱，所以t与阴影强度是成正比的。 最后由于阴影系数越小，阴影越强，所以应该把t / d反过来，变成d / t。然后我们就可以写出代码了。这里我们还添加了系数k，用于调整软阴影强度。本影的return 0.3和半影的max(0.3, )是因为我不想让阴影是一篇死黑，你也可以自行调整。 123456789101112float softshadow(in vec3 ro, in vec3 rd, float mint, float maxt, float k){ float res = 1.0; for (float t=mint; t&lt;maxt;) { float d = getDist(ro + rd * t).x; if (d &lt; 0.001) return 0.3; res = max(0.3, min(res, k * (d / t))); t += d; } return res;} 现在再让我们向渲染函数的最后添加上dif *= softshadow(p, l, 0.02, 10.0, 8.0),就可以看到效果了。。 可以发现我们的软阴影在物体表面表现的不是很好，我打算在后面解决。现在我们可以先作一下弊，让软阴影只在地板上投射，高于地板的地方我们用硬阴影替代或者索性不用阴影。我们对render()稍作修改: 12345678910111213#ifdef FLOOR_GRIDif (p.y &lt; -0.98 || abs(p.x)&gt; 20.0 || abs(p.z) &gt; 20.0)//floor color{ dif -= float((int(p.x+100.0) % 2) ^ (int(p.z+100.0)) % 2) * 0.1; dif *= softshadow(p, l, 0.02, 10.0, 6.0);}else{ //dif *= shadow(p, l, 0.1, 5.0);}#endifreturn dif; 同时我也把k从8调整到了6，增强了软阴影的效果，大家可以对比着看一下。 移动相机我们的相机一直都是固定的，都没有点互动，实在是太无聊了。我们接下来干点有意思的，让我们可以用鼠标转动相机。具体操作其实也该移动旋转SDF差不多，不过我们需要同时移动相机起点和目标像素(uv)。 再放一张图让大家理解一下，。其实我们这里不需要考虑CameraDir，只要直接变换uv就好了。 123456789101112131415161718192021void main(){ mat3x3 rotateX = rotateX((-iMouse.x / iResolution.x) * 20.0); mat3x3 rotateY = rotateY((iMouse.y / iResolution.y) * 2.0); vec2 uv2 = (gl_FragCoord.xy - 0.5f * iResolution.xy) / iResolution.y; vec3 uv = vec3(uv2, 1.0); vec3 cameraP = vec3(0.0, 0.0, 0.0); uv = rotateX * rotateY * uv; cameraP = rotateX * rotateY * cameraP; vec3 trs = vec3(0, 0, -2); cameraP += trs; uv += trs; vec3 rd = normalize(uv - cameraP); vec3 color = render(cameraP, rd, uv.xy); gl_FragColor = vec4(color, 1.0);} 我创建了两个旋转矩阵，把鼠标坐标作为输入，然后把二维的uv2转换成三维的uv,又定义了cameraP。我先旋转了uv和cameraP,再加上了我定义的位移向量trs。那么最后我们渲染时的光线方向就是新的uv - cameraP了，不要忘记正则化。 然后我们的相机就可以跟着我们的鼠标转动了，如果你还想让相机位移，改变trs向量即可。 结尾","link":"/2020/11/10/Shadertoy_vol6/"},{"title":"在Web中添加SoundCloud播放器","text":"今天在逛nitro+官网的时候发现他们的宣传曲都是用的SoundCloud的网页播放器放的，让我有些好奇是怎么做的Google了一下SoundCloud的api, 发现意外的简单。 1.在SoundCloud上找到你想分享的歌曲，例如这首山茶花和Akira Complex的Reality Distortion 2.点击Share按钮，翻到其中Enbed的Tab，然后选择你要的样式 再把其中的Code一栏里的代码复制下来 3.把复制下来的代码粘到你的html文件里。如果你像我一样用的是markdown生成静态页面的框架，你可以在生成完html文件后再添加，或者直接把这段代码复制到你的markdown里(如果它支持在markdown文件中使用html的话) 然后你就可以在你的页面上看到播放器了，像这样子: Akira Complex · かめりあ Vs Akira Complex - Reality Distortion (Original Mix) 是不是非常方便。 SoundCloud的这个功能实在是非常棒。","link":"/2020/07/21/SoundCloud_Share/"},{"title":"网络训练 VGG16 network training","text":"利用Pytorch和CIFAR10数据集训练VGG16网络，附代码 数据集的选取我就选了个CIFAR10,别的懒得整了。代码如下： 123456789101112131415161718transform = transforms.Compose([transforms.Resize((224, 224)), transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)) ])trainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)trainloader = torch.utils.data.DataLoader(trainset, batch_size=4, shuffle=True, num_workers=2)testset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)testloader = torch.utils.data.DataLoader(testset, batch_size=4, shuffle=False, num_workers=2)classes = ('plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck') 拿Windows练的别忘了把num_workers改成0，这个多线程不支持Windows 网络的选择VGG16就是拿来玩玩，他的问题是参数太多，几亿个，节点的参数要占500多MB，训练呢也比较慢。 在使用VGG16来分类CIFAR10的时候呢，要注意一下输入和输出。因为VGG16一开始是用来给ImageNet比赛用的，所以图像尺寸是224*224，输出是1000个分类。 CIFAR10是32*32的图片，分类是10种。所以我对CIFAR10的图像进行了插值，放大到了224*224，同时把最后的full connect layer的大小换成了10。 别的网络结构我就没有改动了，大家在做的时候可以考虑更改下网络来对CIFAR10优化一下。 网络搭建因为是自己练手，没用torch里自带的VGG16网络，还是一层层搭的，代码如下： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667class Net(nn.Module): def __init__(self): super(Net, self).__init__() self.pool = nn.MaxPool2d(2, 2) self.batchNorm1 = nn.BatchNorm2d(64) self.batchNorm2 = nn.BatchNorm2d(128) self.batchNorm3 = nn.BatchNorm2d(256) self.batchNorm4 = nn.BatchNorm2d(512) self.batchNorm5 = nn.BatchNorm2d(512) self.conv1_1 = nn.Conv2d(3, 64, 3, padding=1, bias=False) self.conv1_2 = nn.Conv2d(64, 64, 3, padding=1, bias=False) self.conv2_1 = nn.Conv2d(64, 128, 3, padding=1, bias=False) self.conv2_2 = nn.Conv2d(128, 128, 3, padding=1, bias=False) self.conv3_1 = nn.Conv2d(128, 256, 3, padding=1, bias=False) self.conv3_2 = nn.Conv2d(256, 256, 3, padding=1, bias=False) self.conv3_3 = nn.Conv2d(256, 256, 3, padding=1, bias=False) self.conv4_1 = nn.Conv2d(256, 512, 3, padding=1, bias=False) self.conv4_2 = nn.Conv2d(512, 512, 3, padding=1, bias=False) self.conv4_3 = nn.Conv2d(512, 512, 3, padding=1, bias=False) self.conv5_1 = nn.Conv2d(512, 512, 3, padding=1, bias=False) self.conv5_2 = nn.Conv2d(512, 512, 3, padding=1, bias=False) self.conv5_3 = nn.Conv2d(512, 512, 3, padding=1, bias=False) self.fc1 = nn.Linear(512 * 7 * 7, 4096) self.fc2 = nn.Linear(4096, 4096) self.fc3 = nn.Linear(4096, 10) self.drop = nn.Dropout(p=0.5) def forward(self, x): x = F.relu(self.batchNorm1(self.conv1_1(x))) x = F.relu(self.batchNorm1(self.conv1_2(x))) x = self.pool(x) x = F.relu(self.batchNorm2(self.conv2_1(x))) x = F.relu(self.batchNorm2(self.conv2_2(x))) x = self.pool(x) x = F.relu(self.batchNorm3(self.conv3_1(x))) x = F.relu(self.batchNorm3(self.conv3_2(x))) x = F.relu(self.batchNorm3(self.conv3_3(x))) x = self.pool(x) x = F.relu(self.batchNorm4(self.conv4_1(x))) x = F.relu(self.batchNorm4(self.conv4_2(x))) x = F.relu(self.batchNorm4(self.conv4_3(x))) x = self.pool(x) x = F.relu(self.batchNorm5(self.conv5_1(x))) x = F.relu(self.batchNorm5(self.conv5_2(x))) x = F.relu(self.batchNorm5(self.conv5_3(x))) x = self.pool(x) x = x.view(-1, 512 * 7 * 7) x = F.relu(self.fc1(x)) # x = self.drop(x) x = F.relu(self.fc2(x)) # x = self.drop(x) x = self.fc3(x) return xnet = Net() 里面注释掉的两个x = self.drop(x)是用来防止网络过拟合的，不过加了这玩意训练的时候loss下降的实在太慢了，跑了两个epoch之后就放弃用它了。 训练过程Learning rate 和 momentum分别是0.001和0.9，非常常见的设置。 1234567891011121314151617181920212223device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")PATH = './cifar_VGGnet.pth'net.to(device)for epoch in range(8): running_loss = 0.0 for i, data in enumerate(trainloader, 0): inputs, labels = data[0].to(device), data[1].to(device) optimizer.zero_grad() outputs = net(inputs) loss = criterion(outputs, labels) loss.backward() optimizer.step() running_loss += loss.item() if i % 200 == 199: print('[%d, %5d] loss: %.3f' % (epoch + 1, i + 1, running_loss / 200)) running_loss = 0.0 torch.save(net.state_dict(), PATH)print('Finished Training') 然后你就可以愉快的跑起来了,我的1660Ti跑一个epoch大概要23分钟，一共跑了十几个，最后loss从2.303减少到了0.078。 测试结果123456789101112131415correct = 0total = 0with torch.no_grad(): for data in testloader: images, labels = data[0].to(device), data[1].to(device) outputs = net(images) _, predicted = torch.max(outputs.data, 1) total += labels.size(0) correct += (predicted == labels).sum().item() if total % 400 == 0: print(100 * correct / total)print('Accuracy of the network on the 10000 test images: %d %%' % ( 100 * correct / total)) 1Accuracy of the network on the 10000 test images: 83 % 在网上看了看别人的VGG16分类CIFAR10的准确率可以达到90%，不过他训练了40代，由于时间关系，我就没有接着训练了，毕竟只是个练手项目。 完整代码可见VGG-Net-16 Silentroom · Angel Echo","link":"/2020/07/20/VGG_16/"}],"tags":[{"name":"Games","slug":"Games","link":"/tags/Games/"},{"name":"Graphics","slug":"Graphics","link":"/tags/Graphics/"},{"name":"水","slug":"水","link":"/tags/%E6%B0%B4/"},{"name":"Ray Marching","slug":"Ray-Marching","link":"/tags/Ray-Marching/"},{"name":"Web","slug":"Web","link":"/tags/Web/"},{"name":"Deep Learning","slug":"Deep-Learning","link":"/tags/Deep-Learning/"}],"categories":[{"name":"Tutorials","slug":"Tutorials","link":"/categories/Tutorials/"},{"name":"Miscellaneous","slug":"Miscellaneous","link":"/categories/Miscellaneous/"}]}