{"pages":[],"posts":[{"title":"My first post","text":"为什么爷要写博客？ 说真的，为什么呢？我也不知道 估计过不了多久就不写了，毕竟也没什么可写的 2020.7.8","link":"/2020/07/08/My_first_post/"},{"title":"DIY IIDX Controller","text":"1. IntroThis repo includes CAD files and arduino scripts to help you build your own IIDX controller with relatively low budget. You can access my repository at my Github page: IIDX-DIY For the case of the controllerThe case is build with acrylic. You will need to find a factory and provide them with the CAD files. After getting them all cutted, you can glue them together. Some parts will also need screws to be assembled, you need to buy them in stores. For the circuit of the controllerWe are using arduino as the basic control device here. For my own controller, I am using the arduino micro, but of course you can choose other type of arduino, but to ensure they have enough ports. For building the complete circuit, the basic tools you will need are wires, soldering tin, and electric soldering iron. Besides, the bottons for the controller, the connection cables will also be needed. Since you could hardly find an excactly same type of the botton with the IIDX arcade, I will recommend you to search arcade botton on AliExpress. The encoder is also needed for the rotating part. 2. Let’s begin!!!Building the caseAfter you have get the cutted acrylic, you can assemble them according to the diagram below. notice that there are special glue for acrylic, it can dissolve the acrylic so the case would be in better quality. The bottom plank of the case should not be glued, be careful. The extra hole on the top panel of the box is for moving the disk futher to attain the arcade distance standard. And for you can moving the disk anytime you want, you should use metal columns and glue to assemble the support for the disk. Building the circuitThe circuit part is based on arduino. 1 encoder, 9 bottons, wires, and a long microUSB cable are needed. Though you can use different model of arduino, I recommend you to use the arduino micro, since it could be powered directly by the microUSB port. For the wiring, I soldered my arduino on a board with holes and use wire to prolong the position of connecting the bottons. &nbsp &nbsp &nbsp &nbsp I used port 4, 5, 7, 8, 9, 10, 14, 15 and 16 on arduino for connecting 9 bottons. The encoder used port 2 and 3 It’s necessary for encoder to use the 2 and 3 ports for special function. Do not change them! And you will have to give all the bottons a common ground, which is GND port on arduino and give encoder a 5V and GND. My controller’s circuit looks like this: Now for the programsThe program will analog a keyboard input. You should have seen my program file in the repo, download an IDE for arduino and just upload that program. If you are using different port or key configuration, remember to change the buttonFunc function. Change this section to give different ports with different keys: buttonFunc(6, &apos;g&apos;); buttonFunc(7, &apos;h&apos;); buttonFunc(8, &apos;j&apos;); buttonFunc(9, &apos;v&apos;); buttonFunc(10, &apos;b&apos;); buttonFunc(14, &apos;n&apos;); buttonFunc(16, &apos;m&apos;); buttonFunc(4, &apos;q&apos;); buttonFunc(15, &apos;e&apos;);One more thing to notice is that the encoder I used is 600 p/r, if you are using different encoder, the sensitivity would change, and you can adjust that by changing the the first parameter in the encoderFunc. encoderFunc(3, KEY_RIGHT_SHIFT, KEY_LEFT_SHIFT); //change the 3 to a larger value for higher sensitivity //Vice versa3. Something elseThe rotate disk of my controller is not fixed to the encoder, so it might drop. If you want to solve that problem, you could change the cad file of the transparent one. Replacing the disk with the part under it. Then you could print this part with the 3D model included in my repo and glue them together. Since 3D printing is in better accuracy, it could be tightly attached to the encoder. Have fun in IIDX! vosaica","link":"/2020/07/12/IIDX_DIY_Controller/"},{"title":"Shadertoy学习 vol2. 相机，漫反射，光源","text":"写在开头这篇教程大量引用了blog.demofox.org的内容，但对其中的部分代码做出了修改，以便在vscode的Shader Toy中运行。本文会对部分实现原理做出更加详细的解释，以方便那些对光线追踪和渲染基础没有足够基础的同学。 原博文的链接在此，有需要的可以去参考。 我的代码也可以拿走下载 点击下载 光追的基本原理 光追的基础便是从摄像机处向每个像素发出射线，并检测射线与物体发生的碰撞。当射线碰到光源时，像素便会被照亮。当然细节要比这复杂很多，下面就让我们开始一步一步的搭建光追渲染器吧。 创建光线对于每一个屏幕上的像素，都会有一条光线从摄像机穿过它，再射向屏幕内部。所以我们的第一步工作便是确定每一条光线的起点和方向。 我们把像素所在的平面的上方向和右方向设为x轴正方向和y轴正方向，摄像机到屏幕的垂直方向则是z轴正方向。(左手坐标系) 在使用glsl写shader时，经常会涉及到的一个概念便是归一化(Normalize)，比如说在使用iResolution获取屏幕像素坐标时(假设1080p)，我们并不希望右下角的坐标(1920, 1080)，而更希望是(1, 1)，代表最边角上的像素。把这个大的区间映射到一个统一的区间便是归一化。 对于我们的渲染器，我们会让屏幕的像素坐标处于-0.5到0.5这个区间，所以我们会用(gl_FragCoord/iResolution.xy) * 2.0f - 1来进行归一化。 我们将相机的坐标放在原点，把目标像素所在平面放在z轴上一单位远的距离，这样我们就可以写出代码: 12345678910111213141516void main(){ // The ray starts at the camera position (the origin) vec3 rayPosition = vec3(0.0f, 0.0f, 0.0f); // calculate coordinates of the ray target on the imaginary pixel plane. // -1 to +1 on x,y axis. 1 unit away on the z axis vec3 rayTarget = vec3((gl_FragCoord.xy/iResolution.xy) * 2.0f - 1.0f,, 1.0f); // calculate a normalized vector for the ray direction. // it's pointing from the ray position to the ray target. vec3 rayDir = normalize(rayTarget - rayPosition); // show the ray direction gl_FragColor = vec4(rayDir, 1.0f);} 渲染几何体在渲染几何体时，我们需要一个函数，可以在给出几何体位置，形状信息和射线方向位置时返回是否碰撞和碰撞距离。 让我们来定义一个碰撞信息的结构体： 1234567struct SRayHitInfo{ float dist; vec3 normal; vec3 albedo; vec3 emissive;}; 其中albedo和emissive是对碰撞物体的颜色和发光的描述，在这一步还用不到。不用刻意理解。 显然，对于不同的几何体，他们的碰撞函数并不相同。在这里，我们会使用TestSphereTrace()和TestQuadTrace()来判断碰撞。这两个碰撞函数的推导比较复杂，这里只会提供代码，有兴趣的同学可以自行谷歌原理。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126float ScalarTriple(vec3 u, vec3 v, vec3 w){ return dot(cross(u, v), w);}bool TestQuadTrace(in vec3 rayPos, in vec3 rayDir, inout SRayHitInfo info, in vec3 a, in vec3 b, in vec3 c, in vec3 d){ // calculate normal and flip vertices order if needed vec3 normal = normalize(cross(c-a, c-b)); if (dot(normal, rayDir) &gt; 0.0f) { normal *= -1.0f; vec3 temp = d; d = a; a = temp; temp = b; b = c; c = temp; } vec3 p = rayPos; vec3 q = rayPos + rayDir; vec3 pq = q - p; vec3 pa = a - p; vec3 pb = b - p; vec3 pc = c - p; // determine which triangle to test against by testing against diagonal first vec3 m = cross(pc, pq); float v = dot(pa, m); vec3 intersectPos; if (v &gt;= 0.0f) { // test against triangle a,b,c float u = -dot(pb, m); if (u &lt; 0.0f) return false; float w = ScalarTriple(pq, pb, pa); if (w &lt; 0.0f) return false; float denom = 1.0f / (u+v+w); u*=denom; v*=denom; w*=denom; intersectPos = u*a+v*b+w*c; } else { vec3 pd = d - p; float u = dot(pd, m); if (u &lt; 0.0f) return false; float w = ScalarTriple(pq, pa, pd); if (w &lt; 0.0f) return false; v = -v; float denom = 1.0f / (u+v+w); u*=denom; v*=denom; w*=denom; intersectPos = u*a+v*d+w*c; } float dist; if (abs(rayDir.x) &gt; 0.1f) { dist = (intersectPos.x - rayPos.x) / rayDir.x; } else if (abs(rayDir.y) &gt; 0.1f) { dist = (intersectPos.y - rayPos.y) / rayDir.y; } else { dist = (intersectPos.z - rayPos.z) / rayDir.z; } if (dist &gt; c_minimumRayHitTime &amp;&amp; dist &lt; info.dist) { info.dist = dist; info.normal = normal; return true; } return false;}bool TestSphereTrace(in vec3 rayPos, in vec3 rayDir, inout SRayHitInfo info, in vec4 sphere){ //get the vector from the center of this sphere to where the ray begins. vec3 m = rayPos - sphere.xyz; //get the dot product of the above vector and the ray's vector float b = dot(m, rayDir); float c = dot(m, m) - sphere.w * sphere.w; //exit if r's origin outside s (c &gt; 0) and r pointing away from s (b &gt; 0) if(c &gt; 0.0 &amp;&amp; b &gt; 0.0) return false; //calculate discriminant float discr = b * b - c; //a negative discriminant corresponds to ray missing sphere if(discr &lt; 0.0) return false; //ray now found to intersect sphere, compute smallest t value of intersection bool fromInside = false; float dist = -b - sqrt(discr); if (dist &lt; 0.0f) { fromInside = true; dist = -b + sqrt(discr); } if (dist &gt; c_minimumRayHitTime &amp;&amp; dist &lt; info.dist) { info.dist = dist; info.normal = normalize((rayPos+rayDir*dist) - sphere.xyz) * (fromInside ? -1.0f : 1.0f); return true; } return false;} 在计算光线是否与物体发生碰撞了之后，我们需要根据结果来改变光线的颜色值。但在这一步我们可以先来验证一下我们是否成功的检测了光线的碰撞，所以先写出一个碰撞后直接返回物体颜色，不进行其他计算的函数。 12345678910111213141516171819202122232425262728293031323334353637const float c_superFar = 10000.0f;vec3 GetColorForRay(in vec3 rayPos, in vec3 rayDir){ SRayHitInfo hitInfo; hitInfo.dist = c_superFar; vec3 ret = vec3(0.0f, 0.0f, 0.0f); if (TestSphereTrace(rayPos, rayDir, hitInfo, vec4(-10.0f, 0.0f, 20.0f, 1.0f))) { ret = vec3(1.0f, 0.1f, 0.1f); } if (TestSphereTrace(rayPos, rayDir, hitInfo, vec4(0.0f, 0.0f, 20.0f, 1.0f))) { ret = vec3(0.1f, 1.0f, 0.1f); } { vec3 A = vec3(-15.0f, -15.0f, 22.0f); vec3 B = vec3( 15.0f, -15.0f, 22.0f); vec3 C = vec3( 15.0f, 15.0f, 22.0f); vec3 D = vec3(-15.0f, 15.0f, 22.0f); if (TestQuadTrace(rayPos, rayDir, hitInfo, A, B, C, D)) { ret = vec3(0.7f, 0.7f, 0.7f); } } if (TestSphereTrace(rayPos, rayDir, hitInfo, vec4(10.0f, 0.0f, 20.0f, 1.0f))) { ret = vec3(0.1f, 0.1f, 1.0f); } return ret;} 看一下结果: 这里我的背景是红的是因为我当时不小心把ret的初始值设成vec3 (1.0f, 0.0f, 0.0f)了，你得到的背景应该还是黑色的。 比例矫正现在呢出现了一个问题，因为我们的像素所在坐标位置被我们归一化了，所以渲染出来的图形的比例发生了错误。 这里我们在mian()函数里添加两行代码： 123// correct for aspect ratiofloat aspectRatio = iResolution.x / iResolution.y;rayTarget.y /= aspectRatio; FOV我们的相机距离像素平面为1个单位，经过简单的三角计算便可以得到我们的视角为90°。如果我们减小相机与像素平面的距离，那FOV便会变大，反之同理。 我们可以修改以下代码来通过计算相机与平面距离来调整相机FOV： 12345678910111213141516171819202122232425262728const float c_FOVDegrees = 90.0f;void main(){ // The ray starts at the camera position (the origin) vec3 rayPosition = vec3(0.0f, 0.0f, 0.0f); // calculate the camera distance float cameraDistance = 1.0f / tan(c_FOVDegrees * 0.5f * c_pi / 180.0f); // calculate coordinates of the ray target on the imaginary pixel plane. // -1 to +1 on x,y axis. 1 unit away on the z axis vec3 rayTarget = vec3((gl_FragCoord.xy/iResolution.xy) * 2.0f - 1.0f, cameraDistance); // correct for aspect ratio float aspectRatio = iResolution.x / iResolution.y; rayTarget.y /= aspectRatio; // calculate a normalized vector for the ray direction. // it's pointing from the ray position to the ray target. vec3 rayDir = normalize(rayTarget - rayPosition); // raytrace for this pixel vec3 color = GetColorForRay(rayPosition, rayDir); // show the result gl_FragColor = vec4(color, 1.0f);} 当FOV设置到120的时候，图形大概是这样的： 最激动人心的一步，光线追踪1.之前我们在碰撞信息里定义了两个变量： 12vec3 emissive //物体自发光的颜色vec3 albedo //在白光下的颜色 对于普通不发光物体，它的emissive会是0向量，albedo里的x, y, z会描述它的RGB颜色；对于光源，它的albedo会是0向量，emissive的x, y, z则描述它的发光颜色。 在这里的光线追踪并不模拟物理世界的法则(虽然我也很想做PBR，不过对我来说太难了)，我们会简单地将像素默认颜色设为黑色，以及一个白色的throughout，然后定义以下规则： 当光线照射到物体上时，emissive * throughout 将添加到像素的颜色上。 当光线照射到物体上时，throughout 会乘以该物体的 albedo，这会影响接下来的光的颜色。 当光线照射到物体上时，将在随机方向上反射并继续与场景相交 当光线错过所有对象，或者到达 N 次反弹时，将终止。(本程序将N设置为了8，若性能需求太高，可自行调整) 举一个例子：当光线击中白球，反弹又击中红球，再次反弹并击中白光，此时像素应是红色。 也就是说，当光线击中有颜色的物体时，接下来的光线都会乘以该物体的颜色。 2.针对反射，我们这里的反射均为漫反射，在宏观上是不规则的。 为此，我们需要一个随机数生成器来获得反射后的光线方向。我们使用像素位置和当前帧数作为随机种子，让每个像素在每帧都能获得不同的随机数 12// initialize a random number state based on frag coord and frameuint rngState = uint(uint(gl_FragCoord.x) * uint(1973) + uint(gl_FragCoord.y) * uint(9277) + uint(iFrame) * uint(26699)) | uint(1); 把它放置在main()函数内部，再添加其他功能 123456789101112131415161718192021222324uint wang_hash(inout uint seed){ seed = uint(seed ^ uint(61)) ^ uint(seed &gt;&gt; uint(16)); seed *= uint(9); seed = seed ^ (seed &gt;&gt; 4); seed *= uint(0x27d4eb2d); seed = seed ^ (seed &gt;&gt; 15); return seed;} float RandomFloat01(inout uint state){ return float(wang_hash(state)) / 4294967296.0;} vec3 RandomUnitVector(inout uint state){ float z = RandomFloat01(state) * 2.0f - 1.0f; float a = RandomFloat01(state) * c_twopi; float r = sqrt(1.0f - z * z); float x = r * cos(a); float y = r * sin(a); return vec3(x, y, z);} 把之前GetColorForRay()的内容放进TestSceneTrace()内。这时，我们可以完成真正的GetColorForRay()了: 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576void TestSceneTrace(in vec3 rayPos, in vec3 rayDir, inout SRayHitInfo hitInfo){ { vec3 A = vec3(-15.0f, -15.0f, 22.0f); vec3 B = vec3( 15.0f, -15.0f, 22.0f); vec3 C = vec3( 15.0f, 15.0f, 22.0f); vec3 D = vec3(-15.0f, 15.0f, 22.0f); if (TestQuadTrace(rayPos, rayDir, hitInfo, A, B, C, D)) { hitInfo.albedo = vec3(0.7f, 0.7f, 0.7f); hitInfo.emissive = vec3(0.0f, 0.0f, 0.0f); } } if (TestSphereTrace(rayPos, rayDir, hitInfo, vec4(-10.0f, 0.0f, 20.0f, 1.0f))) { hitInfo.albedo = vec3(1.0f, 0.1f, 0.1f); hitInfo.emissive = vec3(0.0f, 0.0f, 0.0f); } if (TestSphereTrace(rayPos, rayDir, hitInfo, vec4(0.0f, 0.0f, 20.0f, 1.0f))) { hitInfo.albedo = vec3(0.1f, 1.0f, 0.1f); hitInfo.emissive = vec3(0.0f, 0.0f, 0.0f); } if (TestSphereTrace(rayPos, rayDir, hitInfo, vec4(10.0f, 0.0f, 20.0f, 1.0f))) { hitInfo.albedo = vec3(0.1f, 0.1f, 1.0f); hitInfo.emissive = vec3(0.0f, 0.0f, 0.0f); } if (TestSphereTrace(rayPos, rayDir, hitInfo, vec4(10.0f, 10.0f, 20.0f, 5.0f))) { hitInfo.albedo = vec3(0.0f, 0.0f, 0.0f); hitInfo.emissive = vec3(1.0f, 0.9f, 0.7f) * 100.0f; } }vec3 GetColorForRay(in vec3 startRayPos, in vec3 startRayDir, inout uint rngState){ // initialize vec3 ret = vec3(0.0f, 0.0f, 0.0f); vec3 throughput = vec3(1.0f, 1.0f, 1.0f); vec3 rayPos = startRayPos; vec3 rayDir = startRayDir; for (int bounceIndex = 0; bounceIndex &lt;= c_numBounces; ++bounceIndex) { // shoot a ray out into the world SRayHitInfo hitInfo; hitInfo.dist = c_superFar; TestSceneTrace(rayPos, rayDir, hitInfo); // if the ray missed, we are done if (hitInfo.dist == c_superFar) break; // update the ray position rayPos = (rayPos + rayDir * hitInfo.dist) + hitInfo.normal * c_rayPosNormalNudge; // calculate new ray direction, in a cosine weighted hemisphere oriented at normal rayDir = normalize(hitInfo.normal + RandomUnitVector(rngState)); // add in emissive lighting ret += hitInfo.emissive * throughput; // update the colorMultiplier throughput *= hitInfo.albedo; } // return pixel color return ret;} 这里用cosine weight hemisphere是因为Lambert cosine rule, 能给出更真实的反射效果，追求深刻理解的可以去自行谷歌。 运行一下，看看效果: 这时的图像只是一堆零散的点是因为每一帧里光线打到的点都不一样，我们需要让像素去显示在所有帧里的平均值。 平均像素值我们可以添加多个通道来平均像素值。这里新建一个文件，把前一个文件选为iChannel0: 1234567#iChannel0 \"file://test3.frag\"void main(){ vec3 color = texture(iChannel0, gl_FragCoord.xy / iResolution.xy).rgb; gl_FragColor = vec4(color, 1.0f); 同时在上一个文件中添加自己作为一个channel: 1#iChannel0 \"self\" 再往main()中添加混合像素值的代码: 12vec3 lastFrameColor = texture(iChannel0, gl_FragCoord.xy / iResolution.xy).rgb;color = mix(lastFrameColor, color, 1.0f / float(iFrame + 1)); 最后我们可以再布置一下场景： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102void TestSceneTrace(in vec3 rayPos, in vec3 rayDir, inout SRayHitInfo hitInfo){ // to move the scene around, since we can't move the camera yet vec3 sceneTranslation = vec3(0.0f, 0.0f, 10.0f); vec4 sceneTranslation4 = vec4(sceneTranslation, 0.0f); // back wall { vec3 A = vec3(-12.6f, -12.6f, 25.0f) + sceneTranslation; vec3 B = vec3( 12.6f, -12.6f, 25.0f) + sceneTranslation; vec3 C = vec3( 12.6f, 12.6f, 25.0f) + sceneTranslation; vec3 D = vec3(-12.6f, 12.6f, 25.0f) + sceneTranslation; if (TestQuadTrace(rayPos, rayDir, hitInfo, A, B, C, D)) { hitInfo.albedo = vec3(0.7f, 0.7f, 0.7f); hitInfo.emissive = vec3(0.0f, 0.0f, 0.0f); } } // floor { vec3 A = vec3(-12.6f, -12.45f, 25.0f) + sceneTranslation; vec3 B = vec3( 12.6f, -12.45f, 25.0f) + sceneTranslation; vec3 C = vec3( 12.6f, -12.45f, 15.0f) + sceneTranslation; vec3 D = vec3(-12.6f, -12.45f, 15.0f) + sceneTranslation; if (TestQuadTrace(rayPos, rayDir, hitInfo, A, B, C, D)) { hitInfo.albedo = vec3(0.7f, 0.7f, 0.7f); hitInfo.emissive = vec3(0.0f, 0.0f, 0.0f); } } // cieling { vec3 A = vec3(-12.6f, 12.5f, 25.0f) + sceneTranslation; vec3 B = vec3( 12.6f, 12.5f, 25.0f) + sceneTranslation; vec3 C = vec3( 12.6f, 12.5f, 15.0f) + sceneTranslation; vec3 D = vec3(-12.6f, 12.5f, 15.0f) + sceneTranslation; if (TestQuadTrace(rayPos, rayDir, hitInfo, A, B, C, D)) { hitInfo.albedo = vec3(0.7f, 0.7f, 0.7f); hitInfo.emissive = vec3(0.0f, 0.0f, 0.0f); } } // left wall { vec3 A = vec3(-12.5f, -12.6f, 25.0f) + sceneTranslation; vec3 B = vec3(-12.5f, -12.6f, 15.0f) + sceneTranslation; vec3 C = vec3(-12.5f, 12.6f, 15.0f) + sceneTranslation; vec3 D = vec3(-12.5f, 12.6f, 25.0f) + sceneTranslation; if (TestQuadTrace(rayPos, rayDir, hitInfo, A, B, C, D)) { hitInfo.albedo = vec3(0.7f, 0.1f, 0.1f); hitInfo.emissive = vec3(0.0f, 0.0f, 0.0f); } } // right wall { vec3 A = vec3( 12.5f, -12.6f, 25.0f) + sceneTranslation; vec3 B = vec3( 12.5f, -12.6f, 15.0f) + sceneTranslation; vec3 C = vec3( 12.5f, 12.6f, 15.0f) + sceneTranslation; vec3 D = vec3( 12.5f, 12.6f, 25.0f) + sceneTranslation; if (TestQuadTrace(rayPos, rayDir, hitInfo, A, B, C, D)) { hitInfo.albedo = vec3(0.1f, 0.7f, 0.1f); hitInfo.emissive = vec3(0.0f, 0.0f, 0.0f); } } // light { vec3 A = vec3(-5.0f, 12.4f, 22.5f) + sceneTranslation; vec3 B = vec3( 5.0f, 12.4f, 22.5f) + sceneTranslation; vec3 C = vec3( 5.0f, 12.4f, 17.5f) + sceneTranslation; vec3 D = vec3(-5.0f, 12.4f, 17.5f) + sceneTranslation; if (TestQuadTrace(rayPos, rayDir, hitInfo, A, B, C, D)) { hitInfo.albedo = vec3(0.0f, 0.0f, 0.0f); hitInfo.emissive = vec3(1.0f, 0.9f, 0.7f) * 20.0f; } } if (TestSphereTrace(rayPos, rayDir, hitInfo, vec4(-9.0f, -9.1f, 18.0f, 3.0f)+sceneTranslation4)) { hitInfo.albedo = vec3(0.9f, 0.9f, 0.75f); hitInfo.emissive = vec3(0.0f, 0.0f, 0.0f); } if (TestSphereTrace(rayPos, rayDir, hitInfo, vec4(0.0f, -9.1f, 23.0f, 3.0f)+sceneTranslation4)) { hitInfo.albedo = vec3(0.9f, 0.75f, 0.9f); hitInfo.emissive = vec3(0.0f, 0.0f, 0.0f); } if (TestSphereTrace(rayPos, rayDir, hitInfo, vec4(5.5f, -9.1f, 20.0f, 3.0f)+sceneTranslation4)) { hitInfo.albedo = vec3(0.75f, 0.9f, 0.9f); hitInfo.emissive = vec3(0.0f, 0.0f, 0.0f); } } 让我们的渲染成果看起来更nb一点: 结语下一期的教程也会根据这框架来做更深入的效果，敬请期待。","link":"/2020/08/07/Shadertoy_vol2/"},{"title":"Shadertoy学习 vol1. 基本介绍与环境搭建","text":"Link for the Seascape在知乎上看到这个海洋的着色器时，我被震撼到了。立马去找了出处，然后就发现了Shadertoy这个网站。虽然之前在学习Unity的Shader Graph的时候也听说过，不过没想到有人在上面实现了这么好的效果。所以呢，自己便开始琢磨琢磨，看看能不能学习一下。 Shadertoy的基本原理在网站里随便点开一个Shader，就能在右边看到它的代码。这些代码是用openGL的着色器语言—GLSL写的，基本的语法跟C++差不多，不过在一些地方会有出入，比如增加了一些向量和数学的方法，还有一些变量修饰符之类的。 Shadertoy并不是一个完整的着色器，它只是一个片元着色器，不涉及顶点的着色和变换，所以在这里你看不见他们导入的模型和Mesh。这里不少3D物体都是靠Ray Marching和SDF函数实现的，所以会和大家导入一个3D模型再编写Material和Shader的流程有些不同。 当然在实际应用时，在片元着色器里实现3D效果的方法基本用不到，如果想要为了掌握3D游戏里的Shader编写，大家还是去UE或者Unity里玩吧，干嘛费这个劲。 在知道了以上这些基本知识后，如果你仍对Shadertoy抱有兴趣的话，就可以接着往后看了 环境搭建Shadertoy依赖的是openGL的图形api，在浏览器中则多半是webGL，在网站中便可以直接使用。点击右上角的新建，就可以制作你自己的Shader了。 那对于一些不想在网页端实验的同学呢，也有很方便的解决方案，就是去vscode里下一个叫做Shader Toy的插件，再下一个对GLSL语言的支持。 不过要注意的是vscode里的Shader Toy和Shadertoy网站上一部分的变量命名会略有不同，大家可以在文档里看一下。 在之后的教程中，我的代码会以vscode当中的Shader Toy作为标准，所以在Shadertoy上是不能直接运行的，要做出一些修改。 在安装好这两个插件后新建一个文件，命名为test1.frag粘贴以下代码： 12345void main(){ vec2 st = gl_FragCoord.xy/iResolution.xy; gl_FragColor = vec4(st.x,st.y,cos(iTime),1.0);} 保存后右键，选择Shader Toy: Show GLSL Preview 然后你应该就能看到一张会变换颜色的图片了。","link":"/2020/08/07/Shadertoy_vol1/"},{"title":"网络训练 VGG16 network training","text":"利用Pytorch和CIFAR10数据集训练VGG16网络，附代码 数据集的选取我就选了个CIFAR10,别的懒得整了。代码如下： 123456789101112131415161718transform = transforms.Compose([transforms.Resize((224, 224)), transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)) ])trainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)trainloader = torch.utils.data.DataLoader(trainset, batch_size=4, shuffle=True, num_workers=2)testset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)testloader = torch.utils.data.DataLoader(testset, batch_size=4, shuffle=False, num_workers=2)classes = ('plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck') 拿Windows练的别忘了把num_workers改成0，这个多线程不支持Windows 网络的选择VGG16就是拿来玩玩，他的问题是参数太多，几亿个，节点的参数要占500多MB，训练呢也比较慢。 在使用VGG16来分类CIFAR10的时候呢，要注意一下输入和输出。因为VGG16一开始是用来给ImageNet比赛用的，所以图像尺寸是224*224，输出是1000个分类。 CIFAR10是32*32的图片，分类是10种。所以我对CIFAR10的图像进行了插值，放大到了224*224，同时把最后的full connect layer的大小换成了10。 别的网络结构我就没有改动了，大家在做的时候可以考虑更改下网络来对CIFAR10优化一下。 网络搭建因为是自己练手，没用torch里自带的VGG16网络，还是一层层搭的，代码如下： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667class Net(nn.Module): def __init__(self): super(Net, self).__init__() self.pool = nn.MaxPool2d(2, 2) self.batchNorm1 = nn.BatchNorm2d(64) self.batchNorm2 = nn.BatchNorm2d(128) self.batchNorm3 = nn.BatchNorm2d(256) self.batchNorm4 = nn.BatchNorm2d(512) self.batchNorm5 = nn.BatchNorm2d(512) self.conv1_1 = nn.Conv2d(3, 64, 3, padding=1, bias=False) self.conv1_2 = nn.Conv2d(64, 64, 3, padding=1, bias=False) self.conv2_1 = nn.Conv2d(64, 128, 3, padding=1, bias=False) self.conv2_2 = nn.Conv2d(128, 128, 3, padding=1, bias=False) self.conv3_1 = nn.Conv2d(128, 256, 3, padding=1, bias=False) self.conv3_2 = nn.Conv2d(256, 256, 3, padding=1, bias=False) self.conv3_3 = nn.Conv2d(256, 256, 3, padding=1, bias=False) self.conv4_1 = nn.Conv2d(256, 512, 3, padding=1, bias=False) self.conv4_2 = nn.Conv2d(512, 512, 3, padding=1, bias=False) self.conv4_3 = nn.Conv2d(512, 512, 3, padding=1, bias=False) self.conv5_1 = nn.Conv2d(512, 512, 3, padding=1, bias=False) self.conv5_2 = nn.Conv2d(512, 512, 3, padding=1, bias=False) self.conv5_3 = nn.Conv2d(512, 512, 3, padding=1, bias=False) self.fc1 = nn.Linear(512 * 7 * 7, 4096) self.fc2 = nn.Linear(4096, 4096) self.fc3 = nn.Linear(4096, 10) self.drop = nn.Dropout(p=0.5) def forward(self, x): x = F.relu(self.batchNorm1(self.conv1_1(x))) x = F.relu(self.batchNorm1(self.conv1_2(x))) x = self.pool(x) x = F.relu(self.batchNorm2(self.conv2_1(x))) x = F.relu(self.batchNorm2(self.conv2_2(x))) x = self.pool(x) x = F.relu(self.batchNorm3(self.conv3_1(x))) x = F.relu(self.batchNorm3(self.conv3_2(x))) x = F.relu(self.batchNorm3(self.conv3_3(x))) x = self.pool(x) x = F.relu(self.batchNorm4(self.conv4_1(x))) x = F.relu(self.batchNorm4(self.conv4_2(x))) x = F.relu(self.batchNorm4(self.conv4_3(x))) x = self.pool(x) x = F.relu(self.batchNorm5(self.conv5_1(x))) x = F.relu(self.batchNorm5(self.conv5_2(x))) x = F.relu(self.batchNorm5(self.conv5_3(x))) x = self.pool(x) x = x.view(-1, 512 * 7 * 7) x = F.relu(self.fc1(x)) # x = self.drop(x) x = F.relu(self.fc2(x)) # x = self.drop(x) x = self.fc3(x) return xnet = Net() 里面注释掉的两个x = self.drop(x)是用来防止网络过拟合的，不过加了这玩意训练的时候loss下降的实在太慢了，跑了两个epoch之后就放弃用它了。 训练过程Learning rate 和 momentum分别是0.001和0.9，非常常见的设置。 1234567891011121314151617181920212223device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")PATH = './cifar_VGGnet.pth'net.to(device)for epoch in range(8): running_loss = 0.0 for i, data in enumerate(trainloader, 0): inputs, labels = data[0].to(device), data[1].to(device) optimizer.zero_grad() outputs = net(inputs) loss = criterion(outputs, labels) loss.backward() optimizer.step() running_loss += loss.item() if i % 200 == 199: print('[%d, %5d] loss: %.3f' % (epoch + 1, i + 1, running_loss / 200)) running_loss = 0.0 torch.save(net.state_dict(), PATH)print('Finished Training') 然后你就可以愉快的跑起来了,我的1660Ti跑一个epoch大概要23分钟，一共跑了十几个，最后loss从2.303减少到了0.078。 测试结果123456789101112131415correct = 0total = 0with torch.no_grad(): for data in testloader: images, labels = data[0].to(device), data[1].to(device) outputs = net(images) _, predicted = torch.max(outputs.data, 1) total += labels.size(0) correct += (predicted == labels).sum().item() if total % 400 == 0: print(100 * correct / total)print('Accuracy of the network on the 10000 test images: %d %%' % ( 100 * correct / total)) 1Accuracy of the network on the 10000 test images: 83 % 在网上看了看别人的VGG16分类CIFAR10的准确率可以达到90%，不过他训练了40代，由于时间关系，我就没有接着训练了，毕竟只是个练手项目。 完整代码可见VGG-Net-16 Silentroom · Angel Echo","link":"/2020/07/20/VGG_16/"},{"title":"在Web中添加SoundCloud播放器","text":"今天在逛nitro+官网的时候发现他们的宣传曲都是用的SoundCloud的网页播放器放的，让我有些好奇是怎么做的Google了一下SoundCloud的api, 发现意外的简单。 1.在SoundCloud上找到你想分享的歌曲，例如这首山茶花和Akira Complex的Reality Distortion 2.点击Share按钮，翻到其中Enbed的Tab，然后选择你要的样式 再把其中的Code一栏里的代码复制下来 3.把复制下来的代码粘到你的html文件里。如果你像我一样用的是markdown生成静态页面的框架，你可以在生成完html文件后再添加，或者直接把这段代码复制到你的markdown里(如果它支持在markdown文件中使用html的话) 然后你就可以在你的页面上看到播放器了，像这样子: Akira Complex · かめりあ Vs Akira Complex - Reality Distortion (Original Mix) 是不是非常方便。 SoundCloud的这个功能实在是非常棒。","link":"/2020/07/21/SoundCloud_Share/"}],"tags":[{"name":"水","slug":"水","link":"/tags/%E6%B0%B4/"},{"name":"Games","slug":"Games","link":"/tags/Games/"},{"name":"Graphics","slug":"Graphics","link":"/tags/Graphics/"},{"name":"Deep Learning","slug":"Deep-Learning","link":"/tags/Deep-Learning/"},{"name":"Web","slug":"Web","link":"/tags/Web/"}],"categories":[{"name":"Miscellaneous","slug":"Miscellaneous","link":"/categories/Miscellaneous/"},{"name":"Tutorials","slug":"Tutorials","link":"/categories/Tutorials/"}]}