{"pages":[],"posts":[{"title":"DIY IIDX Controller","text":"1. IntroThis repo includes CAD files and arduino scripts to help you build your own IIDX controller with relatively low budget. You can access my repository at my Github page: IIDX-DIY For the case of the controllerThe case is build with acrylic. You will need to find a factory and provide them with the CAD files. After getting them all cutted, you can glue them together. Some parts will also need screws to be assembled, you need to buy them in stores. For the circuit of the controllerWe are using arduino as the basic control device here. For my own controller, I am using the arduino micro, but of course you can choose other type of arduino, but to ensure they have enough ports. For building the complete circuit, the basic tools you will need are wires, soldering tin, and electric soldering iron. Besides, the bottons for the controller, the connection cables will also be needed. Since you could hardly find an excactly same type of the botton with the IIDX arcade, I will recommend you to search arcade botton on AliExpress. The encoder is also needed for the rotating part. 2. Let’s begin!!!Building the caseAfter you have get the cutted acrylic, you can assemble them according to the diagram below. notice that there are special glue for acrylic, it can dissolve the acrylic so the case would be in better quality. The bottom plank of the case should not be glued, be careful. The extra hole on the top panel of the box is for moving the disk futher to attain the arcade distance standard. And for you can moving the disk anytime you want, you should use metal columns and glue to assemble the support for the disk. Building the circuitThe circuit part is based on arduino. 1 encoder, 9 bottons, wires, and a long microUSB cable are needed. Though you can use different model of arduino, I recommend you to use the arduino micro, since it could be powered directly by the microUSB port. For the wiring, I soldered my arduino on a board with holes and use wire to prolong the position of connecting the bottons. &nbsp &nbsp &nbsp &nbsp I used port 4, 5, 7, 8, 9, 10, 14, 15 and 16 on arduino for connecting 9 bottons. The encoder used port 2 and 3 It’s necessary for encoder to use the 2 and 3 ports for special function. Do not change them! And you will have to give all the bottons a common ground, which is GND port on arduino and give encoder a 5V and GND. My controller’s circuit looks like this: Now for the programsThe program will analog a keyboard input. You should have seen my program file in the repo, download an IDE for arduino and just upload that program. If you are using different port or key configuration, remember to change the buttonFunc function. Change this section to give different ports with different keys: buttonFunc(6, &apos;g&apos;); buttonFunc(7, &apos;h&apos;); buttonFunc(8, &apos;j&apos;); buttonFunc(9, &apos;v&apos;); buttonFunc(10, &apos;b&apos;); buttonFunc(14, &apos;n&apos;); buttonFunc(16, &apos;m&apos;); buttonFunc(4, &apos;q&apos;); buttonFunc(15, &apos;e&apos;);One more thing to notice is that the encoder I used is 600 p/r, if you are using different encoder, the sensitivity would change, and you can adjust that by changing the the first parameter in the encoderFunc. encoderFunc(3, KEY_RIGHT_SHIFT, KEY_LEFT_SHIFT); //change the 3 to a larger value for higher sensitivity //Vice versa3. Something elseThe rotate disk of my controller is not fixed to the encoder, so it might drop. If you want to solve that problem, you could change the cad file of the transparent one. Replacing the disk with the part under it. Then you could print this part with the 3D model included in my repo and glue them together. Since 3D printing is in better accuracy, it could be tightly attached to the encoder. Have fun in IIDX! vosaica","link":"/2020/07/12/IIDX_DIY_Controller/"},{"title":"My first post","text":"为什么爷要写博客？ 说真的，为什么呢？我也不知道 估计过不了多久就不写了，毕竟也没什么可写的 2020.7.8","link":"/2020/07/08/My_first_post/"},{"title":"在Web中添加SoundCloud播放器","text":"今天在逛nitro+官网的时候发现他们的宣传曲都是用的SoundCloud的网页播放器放的，让我有些好奇是怎么做的Google了一下SoundCloud的api, 发现意外的简单。 1.在SoundCloud上找到你想分享的歌曲，例如这首山茶花和Akira Complex的Reality Distortion 2.点击Share按钮，翻到其中Enbed的Tab，然后选择你要的样式 再把其中的Code一栏里的代码复制下来 3.把复制下来的代码粘到你的html文件里。如果你像我一样用的是markdown生成静态页面的框架，你可以在生成完html文件后再添加，或者直接把这段代码复制到你的markdown里(如果它支持在markdown文件中使用html的话) 然后你就可以在你的页面上看到播放器了，像这样子: Akira Complex · かめりあ Vs Akira Complex - Reality Distortion (Original Mix) 是不是非常方便。 SoundCloud的这个功能实在是非常棒。","link":"/2020/07/21/SoundCloud_Share/"},{"title":"网络训练 VGG16 network training","text":"利用Pytorch和CIFAR10数据集训练VGG16网络，附代码 数据集的选取我就选了个CIFAR10,别的懒得整了。代码如下： 123456789101112131415161718transform = transforms.Compose([transforms.Resize((224, 224)), transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)) ])trainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)trainloader = torch.utils.data.DataLoader(trainset, batch_size=4, shuffle=True, num_workers=2)testset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)testloader = torch.utils.data.DataLoader(testset, batch_size=4, shuffle=False, num_workers=2)classes = ('plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck') 拿Windows练的别忘了把num_workers改成0，这个多线程不支持Windows 网络的选择VGG16就是拿来玩玩，他的问题是参数太多，几亿个，节点的参数要占500多MB，训练呢也比较慢。 在使用VGG16来分类CIFAR10的时候呢，要注意一下输入和输出。因为VGG16一开始是用来给ImageNet比赛用的，所以图像尺寸是224*224，输出是1000个分类。 CIFAR10是32*32的图片，分类是10种。所以我对CIFAR10的图像进行了插值，放大到了224*224，同时把最后的full connect layer的大小换成了10。 别的网络结构我就没有改动了，大家在做的时候可以考虑更改下网络来对CIFAR10优化一下。 网络搭建因为是自己练手，没用torch里自带的VGG16网络，还是一层层搭的，代码如下： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667class Net(nn.Module): def __init__(self): super(Net, self).__init__() self.pool = nn.MaxPool2d(2, 2) self.batchNorm1 = nn.BatchNorm2d(64) self.batchNorm2 = nn.BatchNorm2d(128) self.batchNorm3 = nn.BatchNorm2d(256) self.batchNorm4 = nn.BatchNorm2d(512) self.batchNorm5 = nn.BatchNorm2d(512) self.conv1_1 = nn.Conv2d(3, 64, 3, padding=1, bias=False) self.conv1_2 = nn.Conv2d(64, 64, 3, padding=1, bias=False) self.conv2_1 = nn.Conv2d(64, 128, 3, padding=1, bias=False) self.conv2_2 = nn.Conv2d(128, 128, 3, padding=1, bias=False) self.conv3_1 = nn.Conv2d(128, 256, 3, padding=1, bias=False) self.conv3_2 = nn.Conv2d(256, 256, 3, padding=1, bias=False) self.conv3_3 = nn.Conv2d(256, 256, 3, padding=1, bias=False) self.conv4_1 = nn.Conv2d(256, 512, 3, padding=1, bias=False) self.conv4_2 = nn.Conv2d(512, 512, 3, padding=1, bias=False) self.conv4_3 = nn.Conv2d(512, 512, 3, padding=1, bias=False) self.conv5_1 = nn.Conv2d(512, 512, 3, padding=1, bias=False) self.conv5_2 = nn.Conv2d(512, 512, 3, padding=1, bias=False) self.conv5_3 = nn.Conv2d(512, 512, 3, padding=1, bias=False) self.fc1 = nn.Linear(512 * 7 * 7, 4096) self.fc2 = nn.Linear(4096, 4096) self.fc3 = nn.Linear(4096, 10) self.drop = nn.Dropout(p=0.5) def forward(self, x): x = F.relu(self.batchNorm1(self.conv1_1(x))) x = F.relu(self.batchNorm1(self.conv1_2(x))) x = self.pool(x) x = F.relu(self.batchNorm2(self.conv2_1(x))) x = F.relu(self.batchNorm2(self.conv2_2(x))) x = self.pool(x) x = F.relu(self.batchNorm3(self.conv3_1(x))) x = F.relu(self.batchNorm3(self.conv3_2(x))) x = F.relu(self.batchNorm3(self.conv3_3(x))) x = self.pool(x) x = F.relu(self.batchNorm4(self.conv4_1(x))) x = F.relu(self.batchNorm4(self.conv4_2(x))) x = F.relu(self.batchNorm4(self.conv4_3(x))) x = self.pool(x) x = F.relu(self.batchNorm5(self.conv5_1(x))) x = F.relu(self.batchNorm5(self.conv5_2(x))) x = F.relu(self.batchNorm5(self.conv5_3(x))) x = self.pool(x) x = x.view(-1, 512 * 7 * 7) x = F.relu(self.fc1(x)) # x = self.drop(x) x = F.relu(self.fc2(x)) # x = self.drop(x) x = self.fc3(x) return xnet = Net() 里面注释掉的两个x = self.drop(x)是用来防止网络过拟合的，不过加了这玩意训练的时候loss下降的实在太慢了，跑了两个epoch之后就放弃用它了。 训练过程Learning rate 和 momentum分别是0.001和0.9，非常常见的设置。 1234567891011121314151617181920212223device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")PATH = './cifar_VGGnet.pth'net.to(device)for epoch in range(8): running_loss = 0.0 for i, data in enumerate(trainloader, 0): inputs, labels = data[0].to(device), data[1].to(device) optimizer.zero_grad() outputs = net(inputs) loss = criterion(outputs, labels) loss.backward() optimizer.step() running_loss += loss.item() if i % 200 == 199: print('[%d, %5d] loss: %.3f' % (epoch + 1, i + 1, running_loss / 200)) running_loss = 0.0 torch.save(net.state_dict(), PATH)print('Finished Training') 然后你就可以愉快的跑起来了,我的1660Ti跑一个epoch大概要23分钟，一共跑了十几个，最后loss从2.303减少到了0.078。 测试结果123456789101112131415correct = 0total = 0with torch.no_grad(): for data in testloader: images, labels = data[0].to(device), data[1].to(device) outputs = net(images) _, predicted = torch.max(outputs.data, 1) total += labels.size(0) correct += (predicted == labels).sum().item() if total % 400 == 0: print(100 * correct / total)print('Accuracy of the network on the 10000 test images: %d %%' % ( 100 * correct / total)) 1Accuracy of the network on the 10000 test images: 83 % 在网上看了看别人的VGG16分类CIFAR10的准确率可以达到90%，不过他训练了40代，由于时间关系，我就没有接着训练了，毕竟只是个练手项目。 完整代码可见VGG-Net-16 Silentroom · Angel Echo","link":"/2020/07/20/VGG_16/"},{"title":"Shadertoy学习 vol1. 基本介绍与环境搭建","text":"Link for the Seascape在知乎上看到这个海洋的着色器时，我被震撼到了。立马去找了出处，然后就发现了Shadertoy这个网站。虽然之前在学习Unity的Shader Graph的时候也听说过，不过没想到有人在上面实现了这么好的效果。所以呢，自己便开始琢磨琢磨，看看能不能学习一下。 Shadertoy的基本原理在网站里随便点开一个Shader，就能在右边看到它的代码。这些代码是用openGL的着色器语言—GLSL写的，基本的语法跟C++差不多，不过在一些地方会有出入，比如增加了一些向量和数学的方法，还有一些变量修饰符之类的。 Shadertoy并不是一个完整的着色器，它只是一个片元着色器，不涉及顶点的着色和变换，所以在这里你看不见他们导入的模型和Mesh。这里不少3D物体都是靠Ray Marching和SDF函数实现的，所以会和大家导入一个3D模型再编写Material和Shader的流程有些不同。 当然在实际应用时，在片元着色器里实现3D效果的方法基本用不到，如果想要为了掌握3D游戏里的Shader编写，大家还是去UE或者Unity里玩吧，干嘛费这个劲。 在知道了以上这些基本知识后，如果你仍对Shadertoy抱有兴趣的话，就可以接着往后看了 环境搭建Shadertoy依赖的是openGL的图形api，在浏览器中则多半是webGL，在网站中便可以直接使用。点击右上角的新建，就可以制作你自己的Shader了。 那对于一些不想在网页端实验的同学呢，也有很方便的解决方案，就是去vscode里下一个叫做Shader Toy的插件，再下一个对GLSL语言的支持。 不过要注意的是vscode里的Shader Toy和Shadertoy网站上一部分的变量命名会略有不同，大家可以在文档里看一下。 在之后的教程中，我的代码会以vscode当中的Shader Toy作为标准，所以在Shadertoy上是不能直接运行的，要做出一些修改。 在安装好这两个插件后新建一个文件，命名为test1.frag粘贴以下代码： 12345void main(){ vec2 st = gl_FragCoord.xy/iResolution.xy; gl_FragColor = vec4(st.x,st.y,cos(iTime),1.0);} 保存后右键，选择Shader Toy: Show GLSL Preview 然后你应该就能看到一张会变换颜色的图片了。","link":"/2020/08/07/Shadertoy_1/"},{"title":"Shadertoy学习 vol3. 抗锯齿，SRGB，镜面反射","text":"书接上文，这次我们来实现更多的渲染功能。本文完整代码 抗锯齿这个内容比较简单，因为我们的渲染器是多帧合成的，所以在每一帧都对像素添加随机的位移就可以了。我们这里随机的位移范围是-0.5到+0.5 找到： 1vec3 rayTarget =vec3((gl_FragCoord.xy/iResolution.xy) * 2.0f - 1.0f, cameraDistance); 改为： 123vec2 jitter = vec2(RandomFloat01(rngState), RandomFloat01(rngState)) - 0.5f;vec3 rayTarget =vec3(((gl_FragCoord.xy + jitter) / iResolution.xy) * 2.0f - 1.0f, cameraDistance); SRGB由于人眼对颜色的感觉不是线性的，所以我们要引入SRGB颜色空间。SRGB里的颜色并不是线性间隔的，深色的值比较多，而浅色的较少。 我们的颜色通道计算不能采用非线性值，所以我们会在最后输出像素颜色时再把颜色转换为SRGB。 这里我们可以在Shadertoy中添加一个commom选项卡，又或者在Shader Toy中新建一个frag文件，他们的作用跟头文件一样。 这里我新建了一个Test4Common.frag，并在其他两个文件的开头添加了如下代码: 1#include \"Test4Common.frag\" 这样，我们的shader便可以调用别的文件里定义的函数了。我们也可以把之前的常量全部移动到这个头文件里。 以下是将线性颜色转换为SRBG颜色的代码: 12345678910111213141516171819202122232425262728293031323334353637vec3 LessThan(vec3 f, float value){ return vec3 ( (f.x &lt; value) ? 1.0f : 0.0f, (f.y &lt; value) ? 1.0f : 0.0f, (f.z &lt; value) ? 1.0f : 0.0f );}vec3 Linear2SRGB(vec3 rgb){ rgb = clamp(rgb, 0.0f, 1.0f); return mix ( pow(rgb, vec3(1.0f / 2.4f)) * 1.055f - 0.055f, rgb * 12.92f, LessThan(rgb, 0.0031308f) );}vec3 SRGB2Linear(vec3 rgb){ return rgb; rgb = clamp(rgb, 0.0f, 1.0f); return mix ( pow(((rgb + 0.055f) / 1.055f), vec3(2.4f)), rgb / 12.92f, LessThan(rgb, 0.04045f) );} 这部分的原理不必过分深究，因为它更多是人为制定的规则，这是微软提供的对颜色空间转换的解释:要记住的是最好不要在渲染时引入SRGB色彩空间，因为有可能会导致渲染出现问题。 曝光和色调映射当我们计算光照的时候，光线的强度可以从0一直到无限，可像素的值却只能停在0到1之间。 为此，我们需要把值重新映射到0到1的区间，使暗处可以保留细节，同时也能看清楚亮处。 我们在头文件当中添加以下代码： 123456789vec3 ACESFilm(vec3 x){ float a = 2.51f; float b = 0.03f; float c = 2.43f; float d = 0.59f; float e = 0.14f; return clamp((x * (a * x + b)) / (x * (c * x + d) + e), 0.0f, 1.0f);} 这个函数其实很好理解，就跟线性插值差不多。 之后再修改主函数: 12color *= c_exposure;color = ACESFilm(color); 注意我们在这里额外添加了一个c_exposure常量，用于控制曝光，我在这里设置的值为0.5. 你也可以手动修改来得到一个满意的亮度。 测试一下，看看效果! 镜面反射现在到了大头了啊，我们要给渲染器加入镜面反射了。 GLSL是有一个reflect()函数已经被定义好的了，你只需要给出入射的光线和法线，就能返回出射光线了。 但在那之前，我们需要对SRayHitInfo的这个结构体做出一些修改。 我们先定义一个SMaterialInfo结构体，把albedo, emissive都移过来，再添加三个新的变量：specularColor，percentSpecular和roughness 再把SMaterialInfo添加到SRayHitInfo里: 123456789101112131415struct SMaterialInfo{ vec3 albedo; vec3 emissive; vec3 specularColor; float percentSpecular; float roughness;};struct SRayHitInfo{ float dist; vec3 normal; SMaterialInfo material;}; specularColor描述了镜面反射后光线的颜色，它和albedo类似，不过一个物体的漫反射颜色和镜面反射颜色不一定相同。 percentageSpecular是一个float，取值范围在0到1之间，描述了镜面反射光线所占全部反射光线的比值，值越低，漫反射所占比重越高，反之亦然 roughness也是取值范围在0到1之间的float，绝对光滑物体的roughness为0。 计算时的原理如下: 我们会随机生成一个0到1之间的小数，如果它小于percentageSpecular,则让光线进行镜面反射，否则进行漫反射。 roughness为1的进行漫反射。 roughness为0的用reflect()计算出射方向。 roughness在0到1之间的就先平方roughness，再线性插值漫反射和镜面反射的方向来算出反射角度，最后再归一化。 对于镜面反射，我们不再乘上albedo，而是specularColor。 其实这个逻辑还是比较好理解的, 然后我们就可以开始写了，找到代码: 1234567rayPos = (rayPos + rayDir * hitInfo.dist) + hitInfo.normal * c_rayPosNormalNudge;rayDir = normalize(hitInfo.normal + RandomUnitVector(rngState));ret += hitInfo.emissive * throughput;throughput *= hitInfo.albedo; 更改为: 1234567891011121314rayPos = (rayPos + rayDir * hitInfo.dist) + hitInfo.normal * c_rayPosNormalNudge; //Wether or not to do the specular reflection ray float doSpecular = (RandomFloat01(rngState) &lt; hitInfo.material.percentSpecular) ? 1.0f : 0.0f; vec3 diffuseRayDir = normalize(hitInfo.normal + RandomUnitVector(rngState)); vec3 specularRayDir = reflect(rayDir, hitInfo.normal); specularRayDir = normalize(mix(specularRayDir, diffuseRayDir, hitInfo.material.roughness * hitInfo.material.roughness)); rayDir = mix(diffuseRayDir, specularRayDir, doSpecular); ret += hitInfo.material.emissive * throughput; throughput *= mix(hitInfo.material.albedo, hitInfo.material.specularColor, doSpecular); 最后更改一下场景，要记得添加上新的Material属性： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179void TestSceneTrace(in vec3 rayPos, in vec3 rayDir, inout SRayHitInfo hitInfo){ // to move the scene around, since we can't move the camera yet vec3 sceneTranslation = vec3(0.0f, 0.0f, 10.0f); vec4 sceneTranslation4 = vec4(sceneTranslation, 0.0f); // back wall { vec3 A = vec3(-12.6f, -12.6f, 25.0f) + sceneTranslation; vec3 B = vec3( 12.6f, -12.6f, 25.0f) + sceneTranslation; vec3 C = vec3( 12.6f, 12.6f, 25.0f) + sceneTranslation; vec3 D = vec3(-12.6f, 12.6f, 25.0f) + sceneTranslation; if (TestQuadTrace(rayPos, rayDir, hitInfo, A, B, C, D)) { hitInfo.material.albedo = vec3(0.7f, 0.7f, 0.7f); hitInfo.material.emissive = vec3(0.0f, 0.0f, 0.0f); hitInfo.material.percentSpecular = 0.0f; hitInfo.material.roughness = 0.0f; hitInfo.material.specularColor = vec3(0.0f, 0.0f, 0.0f); } } // floor { vec3 A = vec3(-12.6f, -12.45f, 25.0f) + sceneTranslation; vec3 B = vec3( 12.6f, -12.45f, 25.0f) + sceneTranslation; vec3 C = vec3( 12.6f, -12.45f, 15.0f) + sceneTranslation; vec3 D = vec3(-12.6f, -12.45f, 15.0f) + sceneTranslation; if (TestQuadTrace(rayPos, rayDir, hitInfo, A, B, C, D)) { hitInfo.material.albedo = vec3(0.7f, 0.7f, 0.7f); hitInfo.material.emissive = vec3(0.0f, 0.0f, 0.0f); hitInfo.material.percentSpecular = 0.0f; hitInfo.material.roughness = 0.0f; hitInfo.material.specularColor = vec3(0.0f, 0.0f, 0.0f); } } // cieling { vec3 A = vec3(-12.6f, 12.5f, 25.0f) + sceneTranslation; vec3 B = vec3( 12.6f, 12.5f, 25.0f) + sceneTranslation; vec3 C = vec3( 12.6f, 12.5f, 15.0f) + sceneTranslation; vec3 D = vec3(-12.6f, 12.5f, 15.0f) + sceneTranslation; if (TestQuadTrace(rayPos, rayDir, hitInfo, A, B, C, D)) { hitInfo.material.albedo = vec3(0.7f, 0.7f, 0.7f); hitInfo.material.emissive = vec3(0.0f, 0.0f, 0.0f); hitInfo.material.percentSpecular = 0.0f; hitInfo.material.roughness = 0.0f; hitInfo.material.specularColor = vec3(0.0f, 0.0f, 0.0f); } } // left wall { vec3 A = vec3(-12.5f, -12.6f, 25.0f) + sceneTranslation; vec3 B = vec3(-12.5f, -12.6f, 15.0f) + sceneTranslation; vec3 C = vec3(-12.5f, 12.6f, 15.0f) + sceneTranslation; vec3 D = vec3(-12.5f, 12.6f, 25.0f) + sceneTranslation; if (TestQuadTrace(rayPos, rayDir, hitInfo, A, B, C, D)) { hitInfo.material.albedo = vec3(0.7f, 0.1f, 0.1f); hitInfo.material.emissive = vec3(0.0f, 0.0f, 0.0f); hitInfo.material.percentSpecular = 0.0f; hitInfo.material.roughness = 0.0f; hitInfo.material.specularColor = vec3(0.0f, 0.0f, 0.0f); } } // right wall { vec3 A = vec3( 12.5f, -12.6f, 25.0f) + sceneTranslation; vec3 B = vec3( 12.5f, -12.6f, 15.0f) + sceneTranslation; vec3 C = vec3( 12.5f, 12.6f, 15.0f) + sceneTranslation; vec3 D = vec3( 12.5f, 12.6f, 25.0f) + sceneTranslation; if (TestQuadTrace(rayPos, rayDir, hitInfo, A, B, C, D)) { hitInfo.material.albedo = vec3(0.1f, 0.7f, 0.1f); hitInfo.material.emissive = vec3(0.0f, 0.0f, 0.0f); hitInfo.material.percentSpecular = 0.0f; hitInfo.material.roughness = 0.0f; hitInfo.material.specularColor = vec3(0.0f, 0.0f, 0.0f); } } // light { vec3 A = vec3(-5.0f, 12.4f, 22.5f) + sceneTranslation; vec3 B = vec3( 5.0f, 12.4f, 22.5f) + sceneTranslation; vec3 C = vec3( 5.0f, 12.4f, 17.5f) + sceneTranslation; vec3 D = vec3(-5.0f, 12.4f, 17.5f) + sceneTranslation; if (TestQuadTrace(rayPos, rayDir, hitInfo, A, B, C, D)) { hitInfo.material.albedo = vec3(0.0f, 0.0f, 0.0f); hitInfo.material.emissive = vec3(1.0f, 0.9f, 0.7f) * 20.0f; hitInfo.material.percentSpecular = 0.0f; hitInfo.material.roughness = 0.0f; hitInfo.material.specularColor = vec3(0.0f, 0.0f, 0.0f); } } if (TestSphereTrace(rayPos, rayDir, hitInfo, vec4(-9.0f, -9.3f, 20.0f, 3.0f)+sceneTranslation4)) { hitInfo.material.albedo = vec3(0.9f, 0.9f, 0.5f); hitInfo.material.emissive = vec3(0.0f, 0.0f, 0.0f); hitInfo.material.percentSpecular = 0.1f; hitInfo.material.roughness = 0.2f; hitInfo.material.specularColor = vec3(0.9f, 0.9f, 0.9f); } if (TestSphereTrace(rayPos, rayDir, hitInfo, vec4(0.0f, -9.3f, 20.0f, 3.0f)+sceneTranslation4)) { hitInfo.material.albedo = vec3(0.9f, 0.5f, 0.9f); hitInfo.material.emissive = vec3(0.0f, 0.0f, 0.0f); hitInfo.material.percentSpecular = 0.3f; hitInfo.material.roughness = 0.2; hitInfo.material.specularColor = vec3(0.9f, 0.9f, 0.9f); } // a ball which has blue diffuse but red specular. an example of a \"bad material\". // a better lighting model wouldn't let you do this sort of thing if (TestSphereTrace(rayPos, rayDir, hitInfo, vec4(9.0f, -9.3f, 20.0f, 3.0f)+sceneTranslation4)) { hitInfo.material.albedo = vec3(0.0f, 0.0f, 1.0f); hitInfo.material.emissive = vec3(0.0f, 0.0f, 0.0f); hitInfo.material.percentSpecular = 0.5f; hitInfo.material.roughness = 0.4f; hitInfo.material.specularColor = vec3(1.0f, 0.0f, 0.0f); } // shiny green balls of varying roughnesses { if (TestSphereTrace(rayPos, rayDir, hitInfo, vec4(-10.0f, 0.0f, 23.0f, 1.75f)+sceneTranslation4)) { hitInfo.material.albedo = vec3(1.0f, 1.0f, 1.0f); hitInfo.material.emissive = vec3(0.0f, 0.0f, 0.0f); hitInfo.material.percentSpecular = 1.0f; hitInfo.material.roughness = 0.0f; hitInfo.material.specularColor = vec3(0.3f, 1.0f, 0.3f); } if (TestSphereTrace(rayPos, rayDir, hitInfo, vec4(-5.0f, 0.0f, 23.0f, 1.75f)+sceneTranslation4)) { hitInfo.material.albedo = vec3(1.0f, 1.0f, 1.0f); hitInfo.material.emissive = vec3(0.0f, 0.0f, 0.0f); hitInfo.material.percentSpecular = 1.0f; hitInfo.material.roughness = 0.25f; hitInfo.material.specularColor = vec3(0.3f, 1.0f, 0.3f); } if (TestSphereTrace(rayPos, rayDir, hitInfo, vec4(0.0f, 0.0f, 23.0f, 1.75f)+sceneTranslation4)) { hitInfo.material.albedo = vec3(1.0f, 1.0f, 1.0f); hitInfo.material.emissive = vec3(0.0f, 0.0f, 0.0f); hitInfo.material.percentSpecular = 1.0f; hitInfo.material.roughness = 0.5f; hitInfo.material.specularColor = vec3(0.3f, 1.0f, 0.3f); } if (TestSphereTrace(rayPos, rayDir, hitInfo, vec4(5.0f, 0.0f, 23.0f, 1.75f)+sceneTranslation4)) { hitInfo.material.albedo = vec3(1.0f, 1.0f, 1.0f); hitInfo.material.emissive = vec3(0.0f, 0.0f, 0.0f); hitInfo.material.percentSpecular = 1.0f; hitInfo.material.roughness = 0.75f; hitInfo.material.specularColor = vec3(0.3f, 1.0f, 0.3f); } if (TestSphereTrace(rayPos, rayDir, hitInfo, vec4(10.0f, 0.0f, 23.0f, 1.75f)+sceneTranslation4)) { hitInfo.material.albedo = vec3(1.0f, 1.0f, 1.0f); hitInfo.material.emissive = vec3(0.0f, 0.0f, 0.0f); hitInfo.material.percentSpecular = 1.0f; hitInfo.material.roughness = 1.0f; hitInfo.material.specularColor = vec3(0.3f, 1.0f, 0.3f); } }} 看一下效果： 真不错 写累了，来首歌放松一下: siqlo · Dual Personality","link":"/2020/08/11/Shadertoy_3/"},{"title":"Shadertoy学习 vol4. Ray Marching 基本原理与实现","text":"","link":"/2020/09/28/Shadertoy_5/"},{"title":"Shadertoy学习 vol4. 菲涅尔, 折射与吸收, 环绕相机","text":"前排提醒，这次的内容会比较多，同时会对之前的代码做出大量修改。另外，这是光追渲染器的最后一篇了，下次估计开坑ray marching。 菲涅尔 简单来说，菲涅尔效应是物体在不同观察角度下，表面反射比率不同的现象。具体的效果取决于物体本身的物理特性。模拟菲涅尔可以增强物体材质的真实感。 下面这个函数实现了菲涅尔效果： 123456789101112131415161718192021float FresnelReflectAmount(float n1, float n2, vec3 normal, vec3 incident, float f0, float f90){ // Schlick aproximation float r0 = (n1-n2) / (n1+n2); r0 *= r0; float cosX = -dot(normal, incident); if (n1 &gt; n2) { float n = n1/n2; float sinT2 = n*n*(1.0-cosX*cosX); // Total internal reflection if (sinT2 &gt; 1.0) return f90; cosX = sqrt(1.0-sinT2); } float x = 1.0-cosX; float ret = r0+(1.0-r0)*x*x*x*x*x; // adjust reflect multiplier for object reflectivity return mix(f0, f90, ret);} 这里n1是入射光线材质的折射率(IOR), n2是被击中的对象材质的折射率,normal是光线碰撞处的表面法线, incident是光线集中对象时的方向, f0是对象的最小反射率(当光线与法线呈0°时), f90是对象的最大反射率(当光线与法线呈90°时). 当应用到我们的渲染器中时，找到这段代码： 12//Wether or not to do the specular reflection ray float doSpecular = (RandomFloat01(rngState) &lt; hitInfo.material.percentSpecular) ? 1.0f : 0.0f; 改为: 123456789101112// apply fresnelfloat specularChance = hitInfo.material.percentSpecular;if (specularChance &gt; 0.0f){ specularChance = FresnelReflectAmount( 1.0, hitInfo.material.IOR, rayDir, hitInfo.normal, hitInfo.material.percentSpecular, 1.0f); } // calculate whether we are going to do a diffuse or specular reflection ray float doSpecular = (RandomFloat01(rngState) &lt; specularChance) ? 1.0f : 0.0f; 注意还要向SMaterialInfo添加IOR 大家可以根据在网上找到的物体IOR来为你的渲染添加更真实的材质: 这是菲涅尔效果的演示，从左到右IOR从1增加到2: 折射和吸收让我们先向SMaterialInfo添加更多的属性，找到结构体，改为： 123456789101112struct SMaterialInfo{ vec3 albedo; vec3 emissive; vec3 specularColor; float specularChance; float specularRoughness; float IOR; float refractionChance; float refractionRoughness; vec3 refractionColor;}; 现在我们有了镜面反射概率，折射概率，以及一个漫反射概率。漫反射概率为1.0 - specularChance - refractionChance, 并没有出现在结构体中，因为我们默认的光线反射方式就是漫反射。 因为我们的SmaterialInfo有了很多属性，很有可能忘记初始化其中一个造成问题，所以要来写一个初始化函数： 1234567891011121314SMaterialInfo GetZeroedMaterial(){ SMaterialInfo ret; ret.albedo = vec3(0.0f, 0.0f, 0.0f); ret.emissive = vec3(0.0f, 0.0f, 0.0f); ret.specularChance = 0.0f; ret.specularRoughness = 0.0f; ret.specularColor = vec3(0.0f, 0.0f, 0.0f); ret.IOR = 1.0f; ret.refractionChance = 0.0f; ret.refractionRoughness = 0.0f; ret.refractionColor = vec3(0.0f, 0.0f, 0.0f); return ret;} 之后每次新建了一个Material都可以调用这个函数来初始化. 接着向SRayHitInfo添加一个新的属性,叫fromInside 1234567struct SRayHitInfo{ float dist; vec3 normal; bool fromInside; SMaterialInfo material;}; 因为我们现在有透明物体, 所以光线会从物体内部碰到表面. 我们需要知道碰撞究竟是在内部发生的还是在外部发生的. 当然, 我们的长方形是没有内部的,所以在TestQuadTrace里把fromInside直接设成false就可以. 不过在TestSphereTrace里还是要修改这个fromInside的值的, 之前代码里已经有判断的逻辑了, 只要再给它赋个值就可以了. 这里用到比尔-朗伯定律来实现的光线衰减, 给光线乘上系数:\\(Multiplier=e^{\\left( -absorb \\cdot distance \\right) }\\) 之后对于逻辑的修改内容比较多, 大家可以直接参考代码来理解, 如有不理解的地方欢迎在评论区留言 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788899091929394959697vec3 GetColorForRay(in vec3 startRayPos, in vec3 startRayDir, inout uint rngState){ vec3 ret = vec3(0.0f, 0.0f, 0.0f); vec3 throughput = vec3(1.0f, 1.0f, 1.0f); vec3 rayPos = startRayPos; vec3 rayDir = startRayDir; for (int bounceIndex = 0; bounceIndex &lt;= c_numBounces; ++bounceIndex) { SRayHitInfo hitInfo; hitInfo.material = GetZeroedMaterial(); hitInfo.dist = c_superFar; hitInfo.fromInside = false; TestSceneTrace(rayPos, rayDir, hitInfo); if (hitInfo.dist == c_superFar) { ret = vec3(0.01f, 0.01f, 0.01f); break; } if (hitInfo.fromInside) throughput *= exp(-hitInfo.material.refractionColor * hitInfo.dist); float specularChance = hitInfo.material.specularChance; float refractionChance = hitInfo.material.refractionChance; float rayProbability = 1.0f; if (specularChance &gt; 0.0f) { specularChance = FresnelReflectAmount ( hitInfo.fromInside ? hitInfo.material.IOR : 1.0, !hitInfo.fromInside ? hitInfo.material.IOR : 1.0, rayDir, hitInfo.normal, hitInfo.material.specularChance, 1.0f ); float chanceMultiplier = (1.0f - specularChance) / (1.0f - hitInfo.material.specularChance); refractionChance *= chanceMultiplier; } float doSpecular = 0.0f; float doRefraction = 0.0f; float raySelectionRoll = RandomFloat01(rngState); //Wether or not to do the specular reflection ray if (specularChance &gt; 0.0f &amp;&amp; raySelectionRoll &lt; specularChance) { doSpecular = 1.0f; rayProbability = specularChance; } else if (refractionChance &gt; 0.0f &amp;&amp; raySelectionRoll &lt; specularChance + refractionChance) { doRefraction = 1.0f; rayProbability = refractionChance; } else { rayProbability = 1.0f - (specularChance + refractionChance); } rayProbability = max(rayProbability, 0.001f); if (doRefraction == 1.0f) rayPos = (rayPos + rayDir * hitInfo.dist) - hitInfo.normal * c_rayPosNormalNudge; else rayPos = (rayPos + rayDir * hitInfo.dist) + hitInfo.normal * c_rayPosNormalNudge; vec3 diffuseRayDir = normalize(hitInfo.normal + RandomUnitVector(rngState)); vec3 specularRayDir = reflect(rayDir, hitInfo.normal); specularRayDir = normalize(mix(specularRayDir, diffuseRayDir, hitInfo.material.specularRoughness * hitInfo.material.specularRoughness)); vec3 refractionRayDir = refract(rayDir, hitInfo.normal, hitInfo.fromInside ? hitInfo.material.IOR : 1.0f / hitInfo.material.IOR); refractionRayDir = normalize(mix(refractionRayDir, normalize(-hitInfo.normal + RandomUnitVector(rngState)), hitInfo.material.refractionRoughness * hitInfo.material.refractionRoughness)); rayDir = mix(diffuseRayDir, specularRayDir, doSpecular); rayDir = mix(rayDir, refractionRayDir, doRefraction); ret += hitInfo.material.emissive * throughput; if (doRefraction == 0.0f) throughput *= mix(hitInfo.material.albedo, hitInfo.material.specularColor, doSpecular); throughput /= rayProbability; { float p = max(throughput.r, max(throughput.g, throughput.b)); if (RandomFloat01(rngState) &gt; p) break; throughput *= 1.0f / p; } } return ret;} 比较值得提及的是在光线刚碰到物体时, 由于我们不知道光线会传播多远, 所以不能立刻计算出吸收的系数. 我们需要等到光线的下一次碰撞, 才能计算出吸收系数. 环绕相机让我们来我们添加一个鼠标移动相机的功能, 把以下代码添加到main()上方: 123456789101112131415161718192021222324252627282930const float c_minCameraAngle = 0.01f;const float c_maxCameraAngle = (c_pi - 0.01f);const vec3 c_cameraAt = vec3(0.0f, 0.0f, 0.0f);const float c_cameraDistance = 20.0f;void GetCameraVectors(out vec3 cameraPos, out vec3 cameraFwd, out vec3 cameraUp, out vec3 cameraRight){ vec2 mouse = iMouse.xy; if (dot(mouse, vec2(1.0f, 1.0f)) == 0.0f) { cameraPos = vec3(0.0f, 0.0f, -c_cameraDistance); cameraFwd = vec3(0.0f, 0.0f, 1.0f); cameraUp = vec3(0.0f, 1.0f, 0.0f); cameraRight = vec3(1.0f, 0.0f, 0.0f); return; } float angleX = -mouse.x * 16.0f / float(iResolution.x); float angleY = mix(c_minCameraAngle, c_maxCameraAngle, mouse.y / float(iResolution)); cameraPos.x = sin(angleX) * sin(angleY) * c_cameraDistance; cameraPos.y = -cos(angleY) * c_cameraDistance; cameraPos.z = cos(angleX) * sin(angleY) * c_cameraDistance; cameraPos += c_cameraAt; cameraFwd = normalize(c_cameraAt - cameraPos); cameraRight = normalize(cross(vec3(0.0f, 1.0f, 0.0f), cameraFwd)); cameraUp = normalize(cross(cameraFwd, cameraRight));} 再修改一下主函数: 12345678910111213141516171819202122232425262728293031323334void main(){ uint rngState = uint(uint(gl_FragCoord.x) * uint(1973) + uint(gl_FragCoord.y) * uint(9277) + uint(iFrame) * uint(26699)) | uint(1); vec3 cameraPos, cameraFwd, cameraUp, cameraRight; GetCameraVectors(cameraPos, cameraFwd, cameraUp, cameraRight); vec2 jitter = vec2(RandomFloat01(rngState), RandomFloat01(rngState)) - 0.5f; vec3 rayDir; { vec2 uvJittered = (gl_FragCoord.xy + jitter) / iResolution.xy; vec2 screen = uvJittered * 2.0f - 1.0f; float aspectRatio = iResolution.x / iResolution.y; screen.y /= aspectRatio; float cameraDistance = tan(c_FOVDegrees * 0.5f * c_pi / 180.0f); rayDir = vec3(screen, cameraDistance); rayDir = normalize(mat3(cameraRight, cameraUp, cameraFwd) * rayDir); } vec3 color = vec3(0.0f, 0.0f, 0.0f); for (int i = 0; i &lt; c_numRendersPerFrame; ++i) color += GetColorForRay(cameraPos, rayDir, rngState) / float(c_numRendersPerFrame); vec4 lastFrameColor = texture(iChannel0, gl_FragCoord.xy / iResolution.xy); float blend = (iFrame &lt; 2 || iMouse.z &gt; 0.0 || lastFrameColor.a == 0.0f || isKeyPressed(32)) ? 1.0f : 1.0f / (1.0f + (1.0f / lastFrameColor.a)); color = mix(lastFrameColor.rgb, color, blend); gl_FragColor = vec4(color, blend);} 好的, 到此就大功告成了!!! (我实在写不动了…) BTW, 没想到Bandcamp也有Embed功能, 来试试吧 EL MAJA by ROBIN JONES SEVEN","link":"/2020/08/20/Shadertoy_4/"},{"title":"Shadertoy学习 vol2. 相机，漫反射，光源","text":"写在开头这篇教程大量引用了blog.demofox.org的内容，但对其中的部分代码做出了修改，以便在vscode的Shader Toy中运行。本文会对部分实现原理做出更加详细的解释，以方便那些对光线追踪和渲染基础没有足够基础的同志。 原博文的链接在此，有需要的可以去参考。 我的代码也可以拿走下载 点击下载 光追的基本原理 光追的基础便是从摄像机处向每个像素发出射线，并检测射线与物体发生的碰撞。当射线碰到光源时，像素便会被照亮。当然细节要比这复杂很多，但你现在已经对原理有了一个基本的认知了，下面就让我们开始一步一步地搭建光追渲染器吧。 创建光线对于每一个屏幕上的像素，都会有一条光线从摄像机穿过它，再射向屏幕内部。所以我们的第一步工作便是确定每一条光线的起点和方向。 我们把像素所在的平面的上方向和右方向设为x轴正方向和y轴正方向，摄像机到屏幕的垂直方向则是z轴正方向。(左手坐标系) 在使用glsl写shader时，经常会涉及到的一个概念便是归一化(Normalize)，比如说在使用iResolution获取屏幕像素坐标时(假设1080p)，我们并不希望右下角的坐标为(1920, 1080)，而更希望是(1, 1)，代表最边角上的像素。把这个大的区间映射到一个统一的区间便是归一化。 对于我们的渲染器，我们会让屏幕的像素坐标处于-1到1这个区间(x轴和y轴都是)，所以我们会用(gl_FragCoord/iResolution.xy) * 2.0f - 1来进行归一化，想象一下函数的位移和缩放。 我们将相机的坐标放在原点，把目标像素所在平面放在z轴上一单位距离远，这样我们就可以写出代码: 12345678910111213141516void main(){ // The ray starts at the camera position (the origin) vec3 rayPosition = vec3(0.0f, 0.0f, 0.0f); // calculate coordinates of the ray target on the imaginary pixel plane. // -1 to +1 on x,y axis. 1 unit away on the z axis vec3 rayTarget = vec3((gl_FragCoord.xy/iResolution.xy) * 2.0f - 1.0f,, 1.0f); // calculate a normalized vector for the ray direction. // it's pointing from the ray position to the ray target. vec3 rayDir = normalize(rayTarget - rayPosition); // show the ray direction gl_FragColor = vec4(rayDir, 1.0f);} 渲染几何体在渲染几何体时，我们需要一个函数，可以在给出几何体位置，形状信息和射线方向位置时返回是否碰撞和碰撞距离。 让我们来定义一个碰撞信息的结构体： 1234567struct SRayHitInfo{ float dist; vec3 normal; vec3 albedo; vec3 emissive;}; 其中albedo和emissive是后面路径追踪时用到的，在这一步还不用刻意理解。 显然，对于不同的几何体，他们的碰撞函数并不相同。在这里，我们会使用TestSphereTrace()和TestQuadTrace()。这两个函数会定义几何体的位置和形状大小，再根据射线位置以及方向来判断碰撞。这两个碰撞函数的推导比较复杂，这里只会提供代码，有兴趣的同学可以自行研究原理。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126float ScalarTriple(vec3 u, vec3 v, vec3 w){ return dot(cross(u, v), w);}bool TestQuadTrace(in vec3 rayPos, in vec3 rayDir, inout SRayHitInfo info, in vec3 a, in vec3 b, in vec3 c, in vec3 d){ // calculate normal and flip vertices order if needed vec3 normal = normalize(cross(c-a, c-b)); if (dot(normal, rayDir) &gt; 0.0f) { normal *= -1.0f; vec3 temp = d; d = a; a = temp; temp = b; b = c; c = temp; } vec3 p = rayPos; vec3 q = rayPos + rayDir; vec3 pq = q - p; vec3 pa = a - p; vec3 pb = b - p; vec3 pc = c - p; // determine which triangle to test against by testing against diagonal first vec3 m = cross(pc, pq); float v = dot(pa, m); vec3 intersectPos; if (v &gt;= 0.0f) { // test against triangle a,b,c float u = -dot(pb, m); if (u &lt; 0.0f) return false; float w = ScalarTriple(pq, pb, pa); if (w &lt; 0.0f) return false; float denom = 1.0f / (u+v+w); u*=denom; v*=denom; w*=denom; intersectPos = u*a+v*b+w*c; } else { vec3 pd = d - p; float u = dot(pd, m); if (u &lt; 0.0f) return false; float w = ScalarTriple(pq, pa, pd); if (w &lt; 0.0f) return false; v = -v; float denom = 1.0f / (u+v+w); u*=denom; v*=denom; w*=denom; intersectPos = u*a+v*d+w*c; } float dist; if (abs(rayDir.x) &gt; 0.1f) { dist = (intersectPos.x - rayPos.x) / rayDir.x; } else if (abs(rayDir.y) &gt; 0.1f) { dist = (intersectPos.y - rayPos.y) / rayDir.y; } else { dist = (intersectPos.z - rayPos.z) / rayDir.z; } if (dist &gt; c_minimumRayHitTime &amp;&amp; dist &lt; info.dist) { info.dist = dist; info.normal = normal; return true; } return false;}bool TestSphereTrace(in vec3 rayPos, in vec3 rayDir, inout SRayHitInfo info, in vec4 sphere){ //get the vector from the center of this sphere to where the ray begins. vec3 m = rayPos - sphere.xyz; //get the dot product of the above vector and the ray's vector float b = dot(m, rayDir); float c = dot(m, m) - sphere.w * sphere.w; //exit if r's origin outside s (c &gt; 0) and r pointing away from s (b &gt; 0) if(c &gt; 0.0 &amp;&amp; b &gt; 0.0) return false; //calculate discriminant float discr = b * b - c; //a negative discriminant corresponds to ray missing sphere if(discr &lt; 0.0) return false; //ray now found to intersect sphere, compute smallest t value of intersection bool fromInside = false; float dist = -b - sqrt(discr); if (dist &lt; 0.0f) { fromInside = true; dist = -b + sqrt(discr); } if (dist &gt; c_minimumRayHitTime &amp;&amp; dist &lt; info.dist) { info.dist = dist; info.normal = normalize((rayPos+rayDir*dist) - sphere.xyz) * (fromInside ? -1.0f : 1.0f); return true; } return false;} 在计算光线是否与物体发生碰撞了之后，我们需要根据结果来改变光线的颜色值。但在这一步我们可以先来验证一下我们是否成功的检测了光线的碰撞，所以先写出一个碰撞后直接返回物体颜色，不进行其他计算的函数。 12345678910111213141516171819202122232425262728293031323334353637const float c_superFar = 10000.0f;vec3 GetColorForRay(in vec3 rayPos, in vec3 rayDir){ SRayHitInfo hitInfo; hitInfo.dist = c_superFar; vec3 ret = vec3(0.0f, 0.0f, 0.0f); if (TestSphereTrace(rayPos, rayDir, hitInfo, vec4(-10.0f, 0.0f, 20.0f, 1.0f))) { ret = vec3(1.0f, 0.1f, 0.1f); } if (TestSphereTrace(rayPos, rayDir, hitInfo, vec4(0.0f, 0.0f, 20.0f, 1.0f))) { ret = vec3(0.1f, 1.0f, 0.1f); } { vec3 A = vec3(-15.0f, -15.0f, 22.0f); vec3 B = vec3( 15.0f, -15.0f, 22.0f); vec3 C = vec3( 15.0f, 15.0f, 22.0f); vec3 D = vec3(-15.0f, 15.0f, 22.0f); if (TestQuadTrace(rayPos, rayDir, hitInfo, A, B, C, D)) { ret = vec3(0.7f, 0.7f, 0.7f); } } if (TestSphereTrace(rayPos, rayDir, hitInfo, vec4(10.0f, 0.0f, 20.0f, 1.0f))) { ret = vec3(0.1f, 0.1f, 1.0f); } return ret;} 看一下结果: 这里我的背景是红的是因为我当时不小心把ret的初始值设成vec3 (1.0f, 0.0f, 0.0f)了，你得到的背景应该还是黑色的。 比例矫正现在出现了一个问题，因为我们的像素所在坐标位置被我们归一化了，所以渲染出来的图形的比例发生了错误，长方形成正方形了。 这里我们在mian()函数里添加两行代码： 123// correct for aspect ratiofloat aspectRatio = iResolution.x / iResolution.y;rayTarget.y /= aspectRatio; 先根据iResolution算出屏幕比例，再改变像素的坐标。 FOV我们的相机距离像素平面为1个单位，经过简单的三角型计算便可以得到我们的视角为90°。如果我们减小相机与像素平面的距离，那FOV便会变大，反之同理。 我们可以修改以下代码来通过计算相机与平面距离来调整相机FOV： 12345678910111213141516171819202122232425262728const float c_FOVDegrees = 90.0f;void main(){ // The ray starts at the camera position (the origin) vec3 rayPosition = vec3(0.0f, 0.0f, 0.0f); // calculate the camera distance float cameraDistance = 1.0f / tan(c_FOVDegrees * 0.5f * c_pi / 180.0f); // calculate coordinates of the ray target on the imaginary pixel plane. // -1 to +1 on x,y axis. 1 unit away on the z axis vec3 rayTarget = vec3((gl_FragCoord.xy/iResolution.xy) * 2.0f - 1.0f, cameraDistance); // correct for aspect ratio float aspectRatio = iResolution.x / iResolution.y; rayTarget.y /= aspectRatio; // calculate a normalized vector for the ray direction. // it's pointing from the ray position to the ray target. vec3 rayDir = normalize(rayTarget - rayPosition); // raytrace for this pixel vec3 color = GetColorForRay(rayPosition, rayDir); // show the result gl_FragColor = vec4(color, 1.0f);} 当FOV设置到120的时候，图形大概是这样的： 最激动人心的一步，光线追踪1.之前我们在碰撞信息里定义了两个变量： 12vec3 emissive //物体自发光的颜色vec3 albedo //在白光下的颜色 对于普通不发光物体，它的emissive会是0向量，albedo里的x, y, z会描述它的RGB颜色；对于光源，它的albedo会是0向量，emissive的x, y, z则描述它的发光颜色。 在这里的光线追踪并不模拟物理世界的法则(虽然我也很想做PBR，不过对我来说太难了)，我们会简单地将像素默认颜色设为黑色，以及一个白色的throughout，然后定义以下规则： 当光线照射到物体上时，emissive * throughout 将添加到像素的颜色上。 当光线照射到物体上时，throughout 会乘以该物体的 albedo，这会影响接下来的光的颜色。 当光线照射到物体上时，将在随机方向上反射并继续与场景相交 当光线错过所有对象，或者到达 N 次反弹时，将终止。(本程序将N设置为了8，若性能需求太高，可自行调整) 举一个例子：当光线击中白球，反弹又击中红球，再次反弹并击中白光，此时像素应是红色。 也就是说，当光线击中有颜色的物体时，接下来的光线都会乘以该物体的颜色。 2.针对反射，我们这里的反射均为漫反射，在宏观上是不规则的。 为此，我们需要一个随机数生成器来获得反射后的光线方向。我们使用像素位置和当前帧数作为随机种子，让每个像素在每帧都能获得不同的随机数 12// initialize a random number state based on frag coord and frameuint rngState = uint(uint(gl_FragCoord.x) * uint(1973) + uint(gl_FragCoord.y) * uint(9277) + uint(iFrame) * uint(26699)) | uint(1); 把它放置在main()函数内部，再添加其他功能 123456789101112131415161718192021222324uint wang_hash(inout uint seed){ seed = uint(seed ^ uint(61)) ^ uint(seed &gt;&gt; uint(16)); seed *= uint(9); seed = seed ^ (seed &gt;&gt; 4); seed *= uint(0x27d4eb2d); seed = seed ^ (seed &gt;&gt; 15); return seed;} float RandomFloat01(inout uint state){ return float(wang_hash(state)) / 4294967296.0;} vec3 RandomUnitVector(inout uint state){ float z = RandomFloat01(state) * 2.0f - 1.0f; float a = RandomFloat01(state) * c_twopi; float r = sqrt(1.0f - z * z); float x = r * cos(a); float y = r * sin(a); return vec3(x, y, z);} 把之前GetColorForRay()的内容放进TestSceneTrace()内。这时，我们可以完成真正的GetColorForRay()了: 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576void TestSceneTrace(in vec3 rayPos, in vec3 rayDir, inout SRayHitInfo hitInfo){ { vec3 A = vec3(-15.0f, -15.0f, 22.0f); vec3 B = vec3( 15.0f, -15.0f, 22.0f); vec3 C = vec3( 15.0f, 15.0f, 22.0f); vec3 D = vec3(-15.0f, 15.0f, 22.0f); if (TestQuadTrace(rayPos, rayDir, hitInfo, A, B, C, D)) { hitInfo.albedo = vec3(0.7f, 0.7f, 0.7f); hitInfo.emissive = vec3(0.0f, 0.0f, 0.0f); } } if (TestSphereTrace(rayPos, rayDir, hitInfo, vec4(-10.0f, 0.0f, 20.0f, 1.0f))) { hitInfo.albedo = vec3(1.0f, 0.1f, 0.1f); hitInfo.emissive = vec3(0.0f, 0.0f, 0.0f); } if (TestSphereTrace(rayPos, rayDir, hitInfo, vec4(0.0f, 0.0f, 20.0f, 1.0f))) { hitInfo.albedo = vec3(0.1f, 1.0f, 0.1f); hitInfo.emissive = vec3(0.0f, 0.0f, 0.0f); } if (TestSphereTrace(rayPos, rayDir, hitInfo, vec4(10.0f, 0.0f, 20.0f, 1.0f))) { hitInfo.albedo = vec3(0.1f, 0.1f, 1.0f); hitInfo.emissive = vec3(0.0f, 0.0f, 0.0f); } if (TestSphereTrace(rayPos, rayDir, hitInfo, vec4(10.0f, 10.0f, 20.0f, 5.0f))) { hitInfo.albedo = vec3(0.0f, 0.0f, 0.0f); hitInfo.emissive = vec3(1.0f, 0.9f, 0.7f) * 100.0f; } }vec3 GetColorForRay(in vec3 startRayPos, in vec3 startRayDir, inout uint rngState){ // initialize vec3 ret = vec3(0.0f, 0.0f, 0.0f); vec3 throughput = vec3(1.0f, 1.0f, 1.0f); vec3 rayPos = startRayPos; vec3 rayDir = startRayDir; for (int bounceIndex = 0; bounceIndex &lt;= c_numBounces; ++bounceIndex) { // shoot a ray out into the world SRayHitInfo hitInfo; hitInfo.dist = c_superFar; TestSceneTrace(rayPos, rayDir, hitInfo); // if the ray missed, we are done if (hitInfo.dist == c_superFar) break; // update the ray position rayPos = (rayPos + rayDir * hitInfo.dist) + hitInfo.normal * c_rayPosNormalNudge; // calculate new ray direction, in a cosine weighted hemisphere oriented at normal rayDir = normalize(hitInfo.normal + RandomUnitVector(rngState)); // add in emissive lighting ret += hitInfo.emissive * throughput; // update the colorMultiplier throughput *= hitInfo.albedo; } // return pixel color return ret;} 这里我们给一开始的光线碰撞距离设置了一个极大值，如果反射8次了碰撞距离还未更新，则认为光线未发生碰撞。这里用cosine weight hemisphere是因为Lambert cosine rule, 能给出更真实的反射效果，追求深刻理解的可以去自行谷歌。 运行一下，看看效果: 这时的图像只是一堆零散的点是因为每一帧里光线打到的点都不一样，我们需要让像素去显示在所有帧里的平均值。 平均像素值我们可以添加多个通道来平均像素值。这里新建一个文件，把前一个文件选为iChannel0: 1234567#iChannel0 \"file://test3.frag\"void main(){ vec3 color = texture(iChannel0, gl_FragCoord.xy / iResolution.xy).rgb; gl_FragColor = vec4(color, 1.0f); 同时在上一个文件中添加自己作为一个channel: 1#iChannel0 \"self\" 再往main()中添加混合像素值的代码: 12vec3 lastFrameColor = texture(iChannel0, gl_FragCoord.xy / iResolution.xy).rgb;color = mix(lastFrameColor, color, 1.0f / float(iFrame + 1)); 最后我们可以再布置一下场景： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102void TestSceneTrace(in vec3 rayPos, in vec3 rayDir, inout SRayHitInfo hitInfo){ // to move the scene around, since we can't move the camera yet vec3 sceneTranslation = vec3(0.0f, 0.0f, 10.0f); vec4 sceneTranslation4 = vec4(sceneTranslation, 0.0f); // back wall { vec3 A = vec3(-12.6f, -12.6f, 25.0f) + sceneTranslation; vec3 B = vec3( 12.6f, -12.6f, 25.0f) + sceneTranslation; vec3 C = vec3( 12.6f, 12.6f, 25.0f) + sceneTranslation; vec3 D = vec3(-12.6f, 12.6f, 25.0f) + sceneTranslation; if (TestQuadTrace(rayPos, rayDir, hitInfo, A, B, C, D)) { hitInfo.albedo = vec3(0.7f, 0.7f, 0.7f); hitInfo.emissive = vec3(0.0f, 0.0f, 0.0f); } } // floor { vec3 A = vec3(-12.6f, -12.45f, 25.0f) + sceneTranslation; vec3 B = vec3( 12.6f, -12.45f, 25.0f) + sceneTranslation; vec3 C = vec3( 12.6f, -12.45f, 15.0f) + sceneTranslation; vec3 D = vec3(-12.6f, -12.45f, 15.0f) + sceneTranslation; if (TestQuadTrace(rayPos, rayDir, hitInfo, A, B, C, D)) { hitInfo.albedo = vec3(0.7f, 0.7f, 0.7f); hitInfo.emissive = vec3(0.0f, 0.0f, 0.0f); } } // cieling { vec3 A = vec3(-12.6f, 12.5f, 25.0f) + sceneTranslation; vec3 B = vec3( 12.6f, 12.5f, 25.0f) + sceneTranslation; vec3 C = vec3( 12.6f, 12.5f, 15.0f) + sceneTranslation; vec3 D = vec3(-12.6f, 12.5f, 15.0f) + sceneTranslation; if (TestQuadTrace(rayPos, rayDir, hitInfo, A, B, C, D)) { hitInfo.albedo = vec3(0.7f, 0.7f, 0.7f); hitInfo.emissive = vec3(0.0f, 0.0f, 0.0f); } } // left wall { vec3 A = vec3(-12.5f, -12.6f, 25.0f) + sceneTranslation; vec3 B = vec3(-12.5f, -12.6f, 15.0f) + sceneTranslation; vec3 C = vec3(-12.5f, 12.6f, 15.0f) + sceneTranslation; vec3 D = vec3(-12.5f, 12.6f, 25.0f) + sceneTranslation; if (TestQuadTrace(rayPos, rayDir, hitInfo, A, B, C, D)) { hitInfo.albedo = vec3(0.7f, 0.1f, 0.1f); hitInfo.emissive = vec3(0.0f, 0.0f, 0.0f); } } // right wall { vec3 A = vec3( 12.5f, -12.6f, 25.0f) + sceneTranslation; vec3 B = vec3( 12.5f, -12.6f, 15.0f) + sceneTranslation; vec3 C = vec3( 12.5f, 12.6f, 15.0f) + sceneTranslation; vec3 D = vec3( 12.5f, 12.6f, 25.0f) + sceneTranslation; if (TestQuadTrace(rayPos, rayDir, hitInfo, A, B, C, D)) { hitInfo.albedo = vec3(0.1f, 0.7f, 0.1f); hitInfo.emissive = vec3(0.0f, 0.0f, 0.0f); } } // light { vec3 A = vec3(-5.0f, 12.4f, 22.5f) + sceneTranslation; vec3 B = vec3( 5.0f, 12.4f, 22.5f) + sceneTranslation; vec3 C = vec3( 5.0f, 12.4f, 17.5f) + sceneTranslation; vec3 D = vec3(-5.0f, 12.4f, 17.5f) + sceneTranslation; if (TestQuadTrace(rayPos, rayDir, hitInfo, A, B, C, D)) { hitInfo.albedo = vec3(0.0f, 0.0f, 0.0f); hitInfo.emissive = vec3(1.0f, 0.9f, 0.7f) * 20.0f; } } if (TestSphereTrace(rayPos, rayDir, hitInfo, vec4(-9.0f, -9.1f, 18.0f, 3.0f)+sceneTranslation4)) { hitInfo.albedo = vec3(0.9f, 0.9f, 0.75f); hitInfo.emissive = vec3(0.0f, 0.0f, 0.0f); } if (TestSphereTrace(rayPos, rayDir, hitInfo, vec4(0.0f, -9.1f, 23.0f, 3.0f)+sceneTranslation4)) { hitInfo.albedo = vec3(0.9f, 0.75f, 0.9f); hitInfo.emissive = vec3(0.0f, 0.0f, 0.0f); } if (TestSphereTrace(rayPos, rayDir, hitInfo, vec4(5.5f, -9.1f, 20.0f, 3.0f)+sceneTranslation4)) { hitInfo.albedo = vec3(0.75f, 0.9f, 0.9f); hitInfo.emissive = vec3(0.0f, 0.0f, 0.0f); } } 定义了5个墙面，3个球体和一个灯光，组成了图形学的Hello World: Cornell box 让我们的渲染结果看起来更nb一点: 这里我的球并没有贴地放置，因为不知道为什么会出现奇怪的反光，虽然肯定跟没有环境光遮蔽无关，不过还得让我再研究研究。 End下一期的教程也会根据这个框架来做更深入的效果，敬请期待。","link":"/2020/08/07/Shadertoy_2/"}],"tags":[{"name":"Games","slug":"Games","link":"/tags/Games/"},{"name":"水","slug":"水","link":"/tags/%E6%B0%B4/"},{"name":"Graphics","slug":"Graphics","link":"/tags/Graphics/"},{"name":"Web","slug":"Web","link":"/tags/Web/"},{"name":"Deep Learning","slug":"Deep-Learning","link":"/tags/Deep-Learning/"},{"name":"Ray Marching","slug":"Ray-Marching","link":"/tags/Ray-Marching/"}],"categories":[{"name":"Tutorials","slug":"Tutorials","link":"/categories/Tutorials/"},{"name":"Miscellaneous","slug":"Miscellaneous","link":"/categories/Miscellaneous/"}]}