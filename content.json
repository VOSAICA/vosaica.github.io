{"pages":[],"posts":[{"title":"DIY IIDX Controller","text":"1. IntroThis repo includes CAD files and arduino scripts to help you build your own IIDX controller with relatively low budget. You can access my repository at my Github page: IIDX-DIY For the case of the controllerThe case is build with acrylic. You will need to find a factory and provide them with the CAD files. After getting them all cutted, you can glue them together. Some parts will also need screws to be assembled, you need to buy them in stores. For the circuit of the controllerWe are using arduino as the basic control device here. For my own controller, I am using the arduino micro, but of course you can choose other type of arduino, but to ensure they have enough ports. For building the complete circuit, the basic tools you will need are wires, soldering tin, and electric soldering iron. Besides, the bottons for the controller, the connection cables will also be needed. Since you could hardly find an excactly same type of the botton with the IIDX arcade, I will recommend you to search arcade botton on AliExpress. The encoder is also needed for the rotating part. 2. Let’s begin!!!Building the caseAfter you have get the cutted acrylic, you can assemble them according to the diagram below. notice that there are special glue for acrylic, it can dissolve the acrylic so the case would be in better quality. The bottom plank of the case should not be glued, be careful. The extra hole on the top panel of the box is for moving the disk futher to attain the arcade distance standard. And for you can moving the disk anytime you want, you should use metal columns and glue to assemble the support for the disk. Building the circuitThe circuit part is based on arduino. 1 encoder, 9 bottons, wires, and a long microUSB cable are needed. Though you can use different model of arduino, I recommend you to use the arduino micro, since it could be powered directly by the microUSB port. For the wiring, I soldered my arduino on a board with holes and use wire to prolong the position of connecting the bottons. &nbsp &nbsp &nbsp &nbsp I used port 4, 5, 7, 8, 9, 10, 14, 15 and 16 on arduino for connecting 9 bottons. The encoder used port 2 and 3 It’s necessary for encoder to use the 2 and 3 ports for special function. Do not change them! And you will have to give all the bottons a common ground, which is GND port on arduino and give encoder a 5V and GND. My controller’s circuit looks like this: Now for the programsThe program will analog a keyboard input. You should have seen my program file in the repo, download an IDE for arduino and just upload that program. If you are using different port or key configuration, remember to change the buttonFunc function. Change this section to give different ports with different keys: buttonFunc(6, &apos;g&apos;); buttonFunc(7, &apos;h&apos;); buttonFunc(8, &apos;j&apos;); buttonFunc(9, &apos;v&apos;); buttonFunc(10, &apos;b&apos;); buttonFunc(14, &apos;n&apos;); buttonFunc(16, &apos;m&apos;); buttonFunc(4, &apos;q&apos;); buttonFunc(15, &apos;e&apos;);One more thing to notice is that the encoder I used is 600 p/r, if you are using different encoder, the sensitivity would change, and you can adjust that by changing the the first parameter in the encoderFunc. encoderFunc(3, KEY_RIGHT_SHIFT, KEY_LEFT_SHIFT); //change the 3 to a larger value for higher sensitivity //Vice versa3. Something elseThe rotate disk of my controller is not fixed to the encoder, so it might drop. If you want to solve that problem, you could change the cad file of the transparent one. Replacing the disk with the part under it. Then you could print this part with the 3D model included in my repo and glue them together. Since 3D printing is in better accuracy, it could be tightly attached to the encoder. Have fun in IIDX! vosaica","link":"/2020/07/12/IIDX_DIY_Controller/"},{"title":"My first post","text":"为什么爷要写博客？ 说真的，为什么呢？我也不知道 估计过不了多久就不写了，毕竟也没什么可写的 2020.7.8","link":"/2020/07/08/My_first_post/"},{"title":"网络训练 VGG16 network training","text":"利用Pytorch和CIFAR10数据集训练VGG16网络，附代码 数据集的选取我就选了个CIFAR10,别的懒得整了。代码如下： 123456789101112131415161718transform = transforms.Compose([transforms.Resize((224, 224)), transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)) ])trainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)trainloader = torch.utils.data.DataLoader(trainset, batch_size=4, shuffle=True, num_workers=2)testset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)testloader = torch.utils.data.DataLoader(testset, batch_size=4, shuffle=False, num_workers=2)classes = ('plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck') 拿Windows练的别忘了把num_workers改成0，这个多线程不支持Windows 网络的选择VGG16就是拿来玩玩，他的问题是参数太多，几亿个，节点的参数要占500多MB，训练呢也比较慢。 在使用VGG16来分类CIFAR10的时候呢，要注意一下输入和输出。因为VGG16一开始是用来给ImageNet比赛用的，所以图像尺寸是224*224，输出是1000个分类。 CIFAR10是32*32的图片，分类是10种。所以我对CIFAR10的图像进行了插值，放大到了224*224，同时把最后的full connect layer的大小换成了10。 别的网络结构我就没有改动了，大家在做的时候可以考虑更改下网络来对CIFAR10优化一下。 网络搭建因为是自己练手，没用torch里自带的VGG16网络，还是一层层搭的，代码如下： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667class Net(nn.Module): def __init__(self): super(Net, self).__init__() self.pool = nn.MaxPool2d(2, 2) self.batchNorm1 = nn.BatchNorm2d(64) self.batchNorm2 = nn.BatchNorm2d(128) self.batchNorm3 = nn.BatchNorm2d(256) self.batchNorm4 = nn.BatchNorm2d(512) self.batchNorm5 = nn.BatchNorm2d(512) self.conv1_1 = nn.Conv2d(3, 64, 3, padding=1, bias=False) self.conv1_2 = nn.Conv2d(64, 64, 3, padding=1, bias=False) self.conv2_1 = nn.Conv2d(64, 128, 3, padding=1, bias=False) self.conv2_2 = nn.Conv2d(128, 128, 3, padding=1, bias=False) self.conv3_1 = nn.Conv2d(128, 256, 3, padding=1, bias=False) self.conv3_2 = nn.Conv2d(256, 256, 3, padding=1, bias=False) self.conv3_3 = nn.Conv2d(256, 256, 3, padding=1, bias=False) self.conv4_1 = nn.Conv2d(256, 512, 3, padding=1, bias=False) self.conv4_2 = nn.Conv2d(512, 512, 3, padding=1, bias=False) self.conv4_3 = nn.Conv2d(512, 512, 3, padding=1, bias=False) self.conv5_1 = nn.Conv2d(512, 512, 3, padding=1, bias=False) self.conv5_2 = nn.Conv2d(512, 512, 3, padding=1, bias=False) self.conv5_3 = nn.Conv2d(512, 512, 3, padding=1, bias=False) self.fc1 = nn.Linear(512 * 7 * 7, 4096) self.fc2 = nn.Linear(4096, 4096) self.fc3 = nn.Linear(4096, 10) self.drop = nn.Dropout(p=0.5) def forward(self, x): x = F.relu(self.batchNorm1(self.conv1_1(x))) x = F.relu(self.batchNorm1(self.conv1_2(x))) x = self.pool(x) x = F.relu(self.batchNorm2(self.conv2_1(x))) x = F.relu(self.batchNorm2(self.conv2_2(x))) x = self.pool(x) x = F.relu(self.batchNorm3(self.conv3_1(x))) x = F.relu(self.batchNorm3(self.conv3_2(x))) x = F.relu(self.batchNorm3(self.conv3_3(x))) x = self.pool(x) x = F.relu(self.batchNorm4(self.conv4_1(x))) x = F.relu(self.batchNorm4(self.conv4_2(x))) x = F.relu(self.batchNorm4(self.conv4_3(x))) x = self.pool(x) x = F.relu(self.batchNorm5(self.conv5_1(x))) x = F.relu(self.batchNorm5(self.conv5_2(x))) x = F.relu(self.batchNorm5(self.conv5_3(x))) x = self.pool(x) x = x.view(-1, 512 * 7 * 7) x = F.relu(self.fc1(x)) # x = self.drop(x) x = F.relu(self.fc2(x)) # x = self.drop(x) x = self.fc3(x) return xnet = Net() 里面注释掉的两个x = self.drop(x)是用来防止网络过拟合的，不过加了这玩意训练的时候loss下降的实在太慢了，跑了两个epoch之后就放弃用它了。 训练过程Learning rate 和 momentum分别是0.001和0.9，非常常见的设置。 1234567891011121314151617181920212223device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")PATH = './cifar_VGGnet.pth'net.to(device)for epoch in range(8): running_loss = 0.0 for i, data in enumerate(trainloader, 0): inputs, labels = data[0].to(device), data[1].to(device) optimizer.zero_grad() outputs = net(inputs) loss = criterion(outputs, labels) loss.backward() optimizer.step() running_loss += loss.item() if i % 200 == 199: print('[%d, %5d] loss: %.3f' % (epoch + 1, i + 1, running_loss / 200)) running_loss = 0.0 torch.save(net.state_dict(), PATH)print('Finished Training') 然后你就可以愉快的跑起来了,我的1660Ti跑一个epoch大概要23分钟，一共跑了十几个，最后loss从2.303减少到了0.078。 测试结果123456789101112131415correct = 0total = 0with torch.no_grad(): for data in testloader: images, labels = data[0].to(device), data[1].to(device) outputs = net(images) _, predicted = torch.max(outputs.data, 1) total += labels.size(0) correct += (predicted == labels).sum().item() if total % 400 == 0: print(100 * correct / total)print('Accuracy of the network on the 10000 test images: %d %%' % ( 100 * correct / total)) 1Accuracy of the network on the 10000 test images: 83 % 在网上看了看别人的VGG16分类CIFAR10的准确率可以达到90%，不过他训练了40代，由于时间关系，我就没有接着训练了，毕竟只是个练手项目。 完整代码可见VGG-Net-16 Silentroom · Angel Echo","link":"/2020/07/20/VGG_16/"},{"title":"在Web中添加SoundCloud播放器","text":"今天在逛nitro+官网的时候发现他们的宣传曲都是用的SoundCloud的网页播放器放的，让我有些好奇是怎么做的 Google了一下SoundCloud的api, 发现意外的简单。 1.在SoundCloud上找到你想分享的歌曲，例如这首山茶花和Akira Complex的Reality Distortion 2.点击Share按钮，翻到其中Enbed的Tab，然后选择你要的样式 再把其中的Code一栏里的代码复制下来 3.把复制下来的代码粘到你的html文件里。如果你像我一样用的是markdown生成静态页面的框架，你可以在生成完html文件后再添加，或者直接把这段代码复制到你的markdown里(如果它支持在markdown文件中使用html的话) 然后你就可以在你的页面上看到播放器了，像这样子: Akira Complex · かめりあ Vs Akira Complex - Reality Distortion (Original Mix) 是不是非常方便。 SoundCloud的这个功能实在是非常棒。","link":"/2020/07/21/SoundCloud_Share/"}],"tags":[{"name":"Games","slug":"Games","link":"/tags/Games/"},{"name":"水","slug":"水","link":"/tags/%E6%B0%B4/"},{"name":"Deep Learning","slug":"Deep-Learning","link":"/tags/Deep-Learning/"},{"name":"Web","slug":"Web","link":"/tags/Web/"}],"categories":[{"name":"Tutorials","slug":"Tutorials","link":"/categories/Tutorials/"},{"name":"Miscellaneous","slug":"Miscellaneous","link":"/categories/Miscellaneous/"}]}